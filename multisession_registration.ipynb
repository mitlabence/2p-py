{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from caiman.base.rois import register_multisession\n",
    "from caiman.utils import visualization\n",
    "from caiman.utils.utils import download_demo\n",
    "\n",
    "from labrotation import file_handling as fh\n",
    "\n",
    "try:\n",
    "    if __IPYTHON__:\n",
    "        get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "        get_ipython().run_line_magic('autoreload', '2')\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open (matlab) files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = []\n",
    "while True:\n",
    "    fpath = fh.open_file(\"Open matlab file, or press Cancel to finish\")\n",
    "    if fpath == \".\":  # user pressed cancel\n",
    "        break\n",
    "    else:\n",
    "        files_list.append(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_list = []\n",
    "A_list = []\n",
    "dims_list = []  # Cn entry in workspace # TODO: A_sparse always have lower resolution, probably from cropping... should I define that as dims?\n",
    "templates = []\n",
    "for fpath in files_list:\n",
    "    mat = scipy.io.loadmat(fpath)\n",
    "    Y = mat[\"caim\"][0][0][0]  # should be Y, at least... Check in matlab (opening the .mat as workspace, check struct entries)\n",
    "    A_sparse = mat[\"caim\"][0][0][1]\n",
    "    FOV = mat[\"caim\"][0][0][14]\n",
    "    dims = mat[\"caim\"][0][0][6].shape\n",
    "\n",
    "    dims_list.append(dims)\n",
    "    Y_list.append(Y)\n",
    "    A_list.append(A_sparse)\n",
    "    templates.append(FOV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make sure that all recordings same dims! \n",
    "for dims in dims_list:\n",
    "    print(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_cropped = []\n",
    "for template in templates:\n",
    "    FOV_shape = template.shape\n",
    "    cropped_shape = dims_list[0]\n",
    "    \n",
    "    x_crop_onesided = (FOV_shape[0] - cropped_shape[0])//2\n",
    "    assert 2*x_crop_onesided == FOV_shape[0] - cropped_shape[0]\n",
    "\n",
    "    y_crop_onesided = (FOV_shape[1] - cropped_shape[1])//2\n",
    "    assert 2*y_crop_onesided == FOV_shape[1] - cropped_shape[1]\n",
    "    template_cropped = template[y_crop_onesided:-y_crop_onesided,x_crop_onesided:-x_crop_onesided]  # TODO: x and y swapped?\n",
    "    templates_cropped.append(template_cropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `register_multisession()`\n",
    "\n",
    "The function `register_multisession()` requires 3 arguments:\n",
    "- `A`: A list of ndarrays or scipy.sparse.csc matrices with (# pixels X # component ROIs) for each session\n",
    "- `dims`: Dimensions of the FOV, needed to restore spatial components to a 2D image\n",
    "- `templates`: List of ndarray matrices of size `dims`, template image of each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_union, assignments, matchings = register_multisession(A=A_list, dims=dims, templates=templates_cropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns 3 variables for further analysis:\n",
    "- `spatial_union`: csc_matrix (# pixels X # total distinct components), the union of all ROIs across all sessions aligned to the FOV of the last session.\n",
    "- `assignments`: ndarray (# total distinct components X # sessions). `assignments[i,j]=k` means that component `k` from session `j` has been identified as component `i` from the union of all components, otherwise it takes a `NaN` value. Note that for each `i` there is at least one session index `j` where `assignments[i,j]!=NaN`.\n",
    "- `matchings`: list of (# sessions) lists. Saves `spatial_union` indices of individual components in each session. `matchings[j][k] = i` means that component `k` from session `j` is represented by component `i` in the union of all components `spatial_union`. In other words `assignments[matchings[j][k], j] = j`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-alignment screening\n",
    "\n",
    "The three outputs can be used to filter components in various ways. For example we can find the components that were active in at least a given a number of sessions. For more examples, check [this script](https://github.com/flatironinstitute/CaImAn/blob/master/use_cases/eLife_scripts/figure_9/Figure_9_alignment.py) that reproduces the results of [Figure 9, as presented in our eLife paper](https://elifesciences.org/articles/38173#fig9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(A_list)  # number of sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(assignments_filtered)):\n",
    "    print(assignments_filtered[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments[0]  # TODO check if there are first nan elements... There should be (i.e. elements that dont appear in the first neuron?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for assignment in assignments:\n",
    "    if np.isnan(assignment[0]):\n",
    "        print(assignment)  # this looks terrible..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: below, all first indices that don't appear in the first recording are converted to 0... \n",
    "# Instead, need to take for each neuron a true or false depending on whether they fulfill the criterion... \n",
    "# And then find the proper spatial component (from one of the recordings where it actually appears) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter components by number of sessions the component could be found\n",
    "\n",
    "n_reg = 2  # minimal number of sessions that each component has to be registered in\n",
    "\n",
    "# Use number of non-NaNs in each row to filter out components that were not registered in enough sessions\n",
    "assignments_filtered = np.array(np.nan_to_num(assignments[np.sum(~np.isnan(assignments), axis=1) >= n_reg]), dtype=int);\n",
    "\n",
    "# Use filtered indices to select the corresponding spatial components\n",
    "spatial_filtered = A_list[0][:, assignments_filtered[:, 0]]\n",
    "\n",
    "# Plot spatial components of the selected components on the template of the last session\n",
    "visualization.plot_contours(spatial_filtered, templates_cropped[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining data of components over multiple sessions (optional)\n",
    "\n",
    "Now that all sessions are aligned and we have a list of re-registered neurons, we can use `assignments` and `matchings` to collect traces from neurons over different sessions.\n",
    "\n",
    "As an exercise, we can collect the traces of all neurons that were registered in all sessions. We already gathered the indices of these neurons in the previous cell in `assignments_filtered`. Assuming that traces of each session are saved in their own `CNMF` object collected in a list, we can iterate through `assignments_filtered` and use these indices to find the re-registered neurons in every session.\n",
    "\n",
    "Note: This notebook does not include the traces of the extracted neurons, only their spatial components. As such the loop below will produce an error if you uncomment it. However, it demonstrates how to use the results of the registration to in your own analysis to extract the traces of the same neurons across different sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traces = np.zeros(assignments_filtered.shape, dtype=np.ndarray)\n",
    "# for i in range(traces.shape[0]):\n",
    "#     for j in range(traces.shape[1]):\n",
    "#         traces[i,j] = cnm_list[j].estimates.C[int(assignments_filtered[i,j])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the array `traces`, where element `traces[i,j] = k` is the temporal component of neuron `i` at session `j`. This can be performed with `F_dff` data or `S` spikes as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
