{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc8f313",
   "metadata": {},
   "source": [
    "Intended use case: given a file with assembled traces, open it, copy data, open new data, append, and write the extended dataset into a new file. Specific use: to add LFP+LabView-only dataset to assembled_traces CHR2 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d0fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auto-reload modules (used to develop functions outside this notebook)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b483ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import labrotation.file_handling as fh\n",
    "import datadoc_util as ddutil\n",
    "from labrotation import two_photon_session as tps\n",
    "import h5py\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import pyabf\n",
    "import matlab.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b96a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datetime_for_fname():\n",
    "    now = datetime.now()\n",
    "    return f\"{now.year:04d}{now.month:02d}{now.day:02d}-{now.hour:02d}{now.minute:02d}{now.second:02d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd6f391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dt(t):\n",
    "    t1 = t[1:]\n",
    "    t0 = t[:-1]\n",
    "    dt = np.zeros(len(t))\n",
    "    dt[1:] = t1 - t0\n",
    "    dt[0] = dt[1]  # assume same step size to avoid 0\n",
    "    return dt\n",
    "def create_totdist_abs(speed, dt):\n",
    "    totdist_abs = np.zeros(len(speed))\n",
    "    totdist_abs[0] = speed[0]*dt[0]\n",
    "    for i in range(1, len(totdist_abs)):\n",
    "        totdist_abs[i] = totdist_abs[i-1] + abs(speed[i]*dt[i])\n",
    "    return totdist_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bcc970",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_dict = dict()\n",
    "if not os.path.exists(\"./.env\"):\n",
    "    print(\".env does not exist\")\n",
    "else:\n",
    "    with open(\"./.env\", \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            l = line.rstrip().split(\"=\")\n",
    "            env_dict[l[0]] = l[1]\n",
    "print(env_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8673ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"SERVER_SYMBOL\" in env_dict.keys():\n",
    "    SERVER_SYMBOL = env_dict[\"SERVER_SYMBOL\"]\n",
    "else:\n",
    "    SERVER_SYMBOL = \"E\"\n",
    "    print(f\"Server symbol not found in .env file. Setting it to {SERVER_SYMBOL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c4f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "matlab_2p_folder = env_dict[\"MATLAB_2P_FOLDER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d237b8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = fh.open_dir(\"Choose export directory for assembled traces!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747adb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_folder = fh.open_dir(\"Choose directory to import results from\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be48235",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddoc = ddutil.DataDocumentation(env_dict[\"DATA_DOCU_FOLDER\"])\n",
    "ddoc.loadDataDoc()\n",
    "ddoc.setDataDriveSymbol(SERVER_SYMBOL)\n",
    "ddoc.checkFileConsistency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b9f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_uuids = []\n",
    "for root, folders, files in os.walk(dsets_dir):\n",
    "    for file in files:\n",
    "        if os.path.splitext(file)[-1] == \".h5\":\n",
    "            original_fname = file.split(\"_segmented\")[0]  # files should be named [original lfp file name]_segmented.h5\n",
    "            #print(original_fname)\n",
    "            #print(ddoc.getUUIDForFile(original_fname + \".abf\"))\n",
    "            existing_uuids.append(ddoc.getUUIDForFile(original_fname + \".abf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57f9d06",
   "metadata": {},
   "source": [
    "## List recordings to be added "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456d0ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_recordings = ddoc.getRecordingsWithExperimentType([\"chr2_ctl_bilat\", \"chr2_sd_bilat\", \"chr2_szsd_bilat\"])\n",
    "# do the filtering by mouse\n",
    "dset_dict = {\"no_nikon\": [\"WEZ-8946\", \"WEZ-8960\"]}\n",
    "\n",
    "df_new_recordings = df_new_recordings[df_new_recordings[\"mouse_id\"].isin(dset_dict[\"no_nikon\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5487b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df_new_recordings.iterrows():\n",
    "    if row.uuid in existing_uuids:\n",
    "        print(f\"{row.uuid}: {row.mouse_id}\\t{row.lfp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba8b03e",
   "metadata": {},
   "source": [
    "## Manually save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f1060",
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid = \"202c1dde3e5d4c07b9033d8c6f82b7ef\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f5e2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ddoc.getSessionFilesForUuid(uuid).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a90d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = files.folder\n",
    "lfp_fpath = os.path.join(fold, files[\"lfp\"])\n",
    "lv_fpath = os.path.join(fold, files[\"labview\"])\n",
    "lv_times_fpath = os.path.join(fold, os.path.splitext(files[\"labview\"])[0]+\"time.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5bee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lv_t = []  # col 0 of ...time.txt files\n",
    "\n",
    "with open(lv_times_fpath, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        lv_t.append(float(line.split(\"\\t\")[0]))\n",
    "lv_t = np.array(lv_t)\n",
    "#lv_t = lv_t - lv_t[0]  # set to 0 starting time\n",
    "#lv_t = lv_t/1000.  # convert to s\n",
    "\n",
    "lv_rounds = []  # col 0\n",
    "lv_speed = []  # col 1\n",
    "lv_totdist = []  # col 2\n",
    "lv_distancepr = []  # col 3\n",
    "lv_stripes = []  # col 6\n",
    "lv_stripespr = []  # col 7\n",
    "with open(lv_fpath, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        if len(line.strip().split(\"\\t\")) == 20:\n",
    "            lv_rounds.append(int(line.strip().split(\"\\t\")[0]))\n",
    "            lv_speed.append(float(line.strip().split(\"\\t\")[1]))\n",
    "            lv_totdist.append(float(line.strip().split(\"\\t\")[2]))\n",
    "            lv_distancepr.append(float(line.strip().split(\"\\t\")[3]))\n",
    "            lv_stripes.append(float(line.strip().split(\"\\t\")[6]))\n",
    "            lv_stripespr.append(float(line.strip().split(\"\\t\")[7]))\n",
    "            \n",
    "        else:\n",
    "            print(len(line.strip().split(\"\\t\")))\n",
    "lv_speed = np.array(lv_speed)\n",
    "lv_rounds = np.array(lv_rounds)\n",
    "lv_totdist = np.array(lv_totdist)\n",
    "lv_distancepr = np.array(lv_distancepr)\n",
    "lv_stripes = np.array(lv_stripes)\n",
    "lv_stripespr = np.array(lv_stripespr)\n",
    "\n",
    "lv_t = lv_t[:len(lv_speed)]  # cut out last, incomplete entry\n",
    "\n",
    "# cut if lv_t shorter\n",
    "lv_speed = lv_speed[:len(lv_t)]\n",
    "lv_rounds = lv_rounds[:len(lv_t)]\n",
    "lv_totdist = lv_totdist[:len(lv_t)]\n",
    "lv_distancepr = lv_distancepr[:len(lv_t)]\n",
    "lv_stripes = lv_stripes[:len(lv_t)]\n",
    "lv_stripespr = lv_stripespr[:len(lv_t)]\n",
    "\n",
    "assert len(lv_t) == len(lv_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf7ee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run part of matlab pipeline\n",
    "belt_struct = {\"speed\":lv_speed, \"distance\": lv_totdist, \"round\":lv_rounds, \"distancePR\": lv_distancepr, \"stripes\": lv_stripes, \"stripesPR\": lv_stripespr, \"time\": lv_t}\n",
    "eng = matlab.engine.start_matlab()\n",
    "m2p_path = eng.genpath(matlab_2p_folder)\n",
    "eng.addpath(m2p_path, nargout=0)\n",
    "belt_struct_proc = eng.beltCorrectWithoutNikon(belt_struct, nargout=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee0922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy \n",
    "belt_dict = dict()\n",
    "for key in belt_struct_proc:   \n",
    "    belt_dict[key] = np.squeeze(np.array(belt_struct_proc[key]))\n",
    "belt_dict[\"time_s\"] = belt_dict[\"time\"]/1000.\n",
    "belt_dict[\"time_s\"] = belt_dict[\"time_s\"] - belt_dict[\"time_s\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ced6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_file = pyabf.ABF(lfp_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701980f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_file.setSweep(sweepNumber=0, channel=0)\n",
    "lfp_t = lfp_file.sweepX\n",
    "lfp_y = lfp_file.sweepY\n",
    "\n",
    "lfp_file.setSweep(sweepNumber=0, channel=1)\n",
    "lfp_loco = lfp_file.sweepY\n",
    "lfp_t = lfp_t*1.0038\n",
    "lfp_t = lfp_t - lfp_t[0]  # shift to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43b2750",
   "metadata": {},
   "outputs": [],
   "source": [
    "(belt_dict[\"time_s\"][-1]-lfp_t[-1])/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e330ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_length = 300  # 5 min\n",
    "stim_length = 4  # 4 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7504ba7a",
   "metadata": {},
   "source": [
    "### Replicate some steps from the matlab processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a9c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_type = ddoc.getExperimentTypeForUuid(uuid)\n",
    "mouse_id = ddoc.getMouseIdForUuid(uuid)\n",
    "win_type = \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f2a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784832bd",
   "metadata": {},
   "source": [
    "### Match two locomotion channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d05566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,18))\n",
    "plt.plot(belt_dict[\"time_s\"]-offset, belt_dict[\"speed\"])\n",
    "plt.plot(lfp_t, lfp_loco-2.5)\n",
    "plt.xlim((0, 100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92beb2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,18))\n",
    "plt.plot(belt_dict[\"time_s\"]-offset, belt_dict[\"speed\"])\n",
    "plt.plot(lfp_t, lfp_loco-2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbf4c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,18))\n",
    "plt.plot(belt_dict[\"time_s\"]-offset, belt_dict[\"speed\"])\n",
    "plt.plot(lfp_t, lfp_y)\n",
    "plt.vlines(x=[bl_length, bl_length+stim_length], ymin=-1, ymax=1, color=\"red\")\n",
    "#plt.xlim((295, 390))\n",
    "#plt.xlim((0, 20))\n",
    "plt.show()\n",
    "# TODO: come up with a constant offset, apply it to all recordings and check roughly if they match. Then decide whether use LFP or LabView (make sure labview > 10 min, so we can use loco quantities)\n",
    "# TODO: then assemble the dataset, include in loco analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e209baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,18))\n",
    "plt.plot(belt_dict[\"time_s\"]-offset, belt_dict[\"speed\"])\n",
    "plt.plot(lfp_t, lfp_y)\n",
    "plt.vlines(x=[bl_length, bl_length+stim_length], ymin=-1, ymax=1, color=\"red\")\n",
    "plt.xlim((295, 390))\n",
    "#plt.xlim((0, 20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee376e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fpath = os.path.join(dsets_dir, os.path.splitext(os.path.split(lfp_fpath)[-1])[0] + \"_segmented.h5\")\n",
    "print(output_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = create_dt(belt_dict[\"time_s\"])\n",
    "totdist_abs = create_totdist_abs(belt_dict[\"speed\"], dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed453d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(output_fpath, \"w\") as hf:\n",
    "    hf.attrs[\"uuid\"] = uuid\n",
    "    hf.attrs[\"stim_start\"] = bl_length\n",
    "    hf.attrs[\"poststim_start\"] = bl_length + stim_length\n",
    "    hf.attrs[\"exp_type\"] = exp_type\n",
    "    hf.attrs[\"win_type\"] = win_type\n",
    "    hf.attrs[\"mouse_id\"] = mouse_id\n",
    "    \n",
    "    hf.create_dataset(\"lfp_mov_t\", data=lfp_t)\n",
    "    hf.create_dataset(\"lfp_mov_y\", data=lfp_loco)    \n",
    "    hf.create_dataset(\"lfp_t\", data=lfp_t)    \n",
    "    hf.create_dataset(\"lfp_y\", data=lfp_y) \n",
    "    \n",
    "    hf.create_dataset(\"lv_dist\", data=belt_dict[\"distancePR\"])     \n",
    "    hf.create_dataset(\"lv_dt\", data=dt) \n",
    "    hf.create_dataset(\"lv_speed\", data=belt_dict[\"speed\"]) \n",
    "    hf.create_dataset(\"lv_running\", data=belt_dict[\"running\"]) \n",
    "    hf.create_dataset(\"lv_t_s\", data=belt_dict[\"time_s\"]-offset) # match to lfp\n",
    "    hf.create_dataset(\"lv_totdist\", data=belt_dict[\"distance\"])  # totdist - where does it come from? Is it distance?\n",
    "    hf.create_dataset(\"lv_totdist_abs\", data=totdist_abs) \n",
    "    hf.create_dataset(\"lv_rounds\", data=belt_dict[\"round\"])\n",
    "    hf.create_dataset(\"lv_stripesPR\", data=belt_dict[\"round\"])\n",
    "    hf.create_dataset(\"lv_distancePR\", data=belt_dict[\"distancePR\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa58fc7",
   "metadata": {},
   "source": [
    "### Run until this point, then go back to specify new UUID to add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4f5d1",
   "metadata": {},
   "source": [
    "# Open all, calculate 15Hz dataset (compatible with recordings including Nikon), save as single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51dbf0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1872607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_attrs = dict()\n",
    "dict_data = dict()\n",
    "\n",
    "for root, dirs, files in os.walk(dset_folder):\n",
    "    for file in files:\n",
    "        fpath = os.path.join(root, file)\n",
    "        assert os.path.exists(fpath)\n",
    "        with h5py.File(fpath, \"r\") as hf:\n",
    "            dict_current_attrs = dict()\n",
    "            for key in hf.attrs.keys():\n",
    "                dict_current_attrs[key] = hf.attrs[key]\n",
    "            uuid = dict_current_attrs[\"uuid\"]\n",
    "            dict_attrs[uuid] = dict_current_attrs\n",
    "            \n",
    "            dict_current_data = dict()\n",
    "            for key in hf.keys():\n",
    "                dict_current_data[key] = hf[key][:]\n",
    "            dict_data[uuid] = dict_current_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb045c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dset = dict()\n",
    "for uuid in dict_data.keys():\n",
    "    print(uuid)\n",
    "    t_last = dict_data[uuid][\"lv_t_s\"][-1]\n",
    "    t_15hz = np.arange(0, t_last, 1/15.)\n",
    "\n",
    "    # lfp starts at 0, change it so that labview starts at 0\n",
    "    lv_t_s = dict_data[uuid][\"lv_t_s\"]\n",
    "    lfp_t = dict_data[uuid][\"lfp_t\"]\n",
    "    lfp_mov_t = dict_data[uuid][\"lfp_mov_t\"]  # should be same as lfp_t\n",
    "    \n",
    "    offset = lv_t_s[0]\n",
    "    lfp_t = lfp_t - offset\n",
    "    lfp_mov_t = lfp_mov_t - offset\n",
    "    lv_t_s = lv_t_s - offset\n",
    "    \n",
    "    downsample_indices = np.searchsorted(lv_t_s, t_15hz, side=\"left\")\n",
    "    # shift first time point to 0 (should not do anything as first entry should be 0)\n",
    "    offset = lv_t_s[downsample_indices[0]]\n",
    "    lv_t_s = lv_t_s - offset\n",
    "    lfp_t = lfp_t - offset\n",
    "    lfp_mov_t = lfp_mov_t - offset\n",
    "    \n",
    "    # define downsampled data, use legacy \"scn\" notation (\"scanner time frame\")\n",
    "    tsscn = lv_t_s[downsample_indices]\n",
    "    lv_dt_scn = create_dt(tsscn)\n",
    "    \n",
    "    lv_dist_scn = dict_data[uuid][\"lv_dist\"][downsample_indices]\n",
    "    lv_rounds_scn = dict_data[uuid][\"lv_rounds\"][downsample_indices]\n",
    "    lv_running_scn = dict_data[uuid][\"lv_running\"][downsample_indices]\n",
    "    lv_speed_scn = dict_data[uuid][\"lv_speed\"][downsample_indices]\n",
    "    lv_totdist_scn = dict_data[uuid][\"lv_totdist\"][downsample_indices]\n",
    "    lv_totdist_abs_scn = dict_data[uuid][\"lv_totdist_abs\"][downsample_indices]\n",
    "    \n",
    "    dict_uuid = dict()\n",
    "    dict_uuid[\"lfp_mov_t\"] = lfp_mov_t\n",
    "    dict_uuid[\"lfp_mov_y\"] =  dict_data[uuid][\"lfp_mov_y\"]\n",
    "    dict_uuid[\"lfp_t\"] = lfp_t\n",
    "    dict_uuid[\"lfp_y\"] = dict_data[uuid][\"lfp_y\"]\n",
    "    dict_uuid[\"lv_dist\"] = lv_dist_scn\n",
    "    dict_uuid[\"lv_dt\"] = lv_dt_scn\n",
    "    dict_uuid[\"lv_rounds\"] = lv_rounds_scn\n",
    "    dict_uuid[\"lv_running\"] = lv_running_scn\n",
    "    dict_uuid[\"lv_speed\"] = lv_speed_scn\n",
    "    dict_uuid[\"lv_t_s\"] = tsscn\n",
    "    dict_uuid[\"lv_totdist\"] = lv_totdist_scn\n",
    "    dict_uuid[\"lv_totdist_abs\"] = lv_totdist_abs_scn\n",
    "    \n",
    "    dict_dset[uuid] = dict_uuid\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab16cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_attrs[\"4b1ee9d07534439a8bc037e6771253f7\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538381ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check dataset lengths\n",
    "for uuid in dict_dset.keys():\n",
    "    lfp_len = len(dict_dset[uuid][\"lfp_mov_t\"])\n",
    "    lv_len = len(dict_dset[uuid][\"lv_t_s\"])\n",
    "    for dname in dict_dset[uuid].keys():\n",
    "        if \"lv_\" in dname:\n",
    "            assert len(dict_dset[uuid][dname]) == lv_len\n",
    "        else:\n",
    "            assert len(dict_dset[uuid][dname]) == lfp_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8753336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to single file of assembled traces\n",
    "out_fpath = os.path.join(output_dir, f\"assembled_traces_bilat_{get_datetime_for_fname()}.h5\")\n",
    "with h5py.File(out_fpath, \"w\") as hf:\n",
    "    for uuid in dict_dset.keys():\n",
    "        uuid_grp = hf.create_group(uuid)\n",
    "\n",
    "        \n",
    "        lv_t_s = dict_dset[uuid][\"lv_t_s\"]\n",
    "        lfp_t = dict_dset[uuid][\"lfp_t\"]\n",
    "        \n",
    "        t_stim = dict_attrs[uuid][\"stim_start\"]\n",
    "        t_poststim = dict_attrs[uuid][\"poststim_start\"]\n",
    "        \n",
    "        i_stim_lv = np.searchsorted(lv_t_s, t_stim)\n",
    "        i_poststim_lv =  np.searchsorted(lv_t_s, t_poststim)\n",
    "        i_stim_lfp = np.searchsorted(lfp_t, t_stim)\n",
    "        i_poststim_lfp = np.searchsorted(lfp_t, t_poststim)\n",
    "        \n",
    "        # add attributes\n",
    "        uuid_grp.attrs[\"uuid\"] = uuid   \n",
    "        uuid_grp.attrs[\"mouse_id\"] = dict_attrs[uuid][\"mouse_id\"]\n",
    "        uuid_grp.attrs[\"window_type\"] = dict_attrs[uuid][\"win_type\"]\n",
    "        uuid_grp.attrs[\"exp_type\"] = dict_attrs[uuid][\"exp_type\"]\n",
    "        uuid_grp.attrs[\"break_points\"] = np.array([0, i_stim_lv, i_poststim_lv])\n",
    "        print(np.array([0, i_stim_lv, i_poststim_lv]))\n",
    "        print(uuid_grp.attrs[\"break_points\"])\n",
    "        uuid_grp.attrs[\"break_points_lfp\"] = np.array([np.searchsorted(lfp_t, 0.),  i_stim_lfp, i_poststim_lfp])\n",
    "        uuid_grp.attrs[\"has_lfp\"] = True\n",
    "        uuid_grp.attrs[\"i_stim_begin_frame\"] = i_stim_lv\n",
    "        uuid_grp.attrs[\"i_stim_begin_frame\"] = i_poststim_lv - 1\n",
    "        uuid_grp.attrs[\"n_bl_frames\"] = i_stim_lv\n",
    "        uuid_grp.attrs[\"n_am_frames\"] = len(lv_t_s) - i_poststim_lv\n",
    "        uuid_grp.attrs[\"n_frames\"] = len(lv_t_s)\n",
    "        uuid_grp.attrs[\"n_lfp_mov_steps\"] = len(lfp_mov_t)\n",
    "        uuid_grp.attrs[\"n_lfp_steps\"] = len(lfp_t)\n",
    "        uuid_grp.attrs[\"recording_break_points\"] = np.array([0])\n",
    "        uuid_grp.attrs[\"recording_break_points_lfp\"] = np.array([0])\n",
    "        uuid_grp.attrs[\"session_uuids\"] = [uuid]\n",
    "        #  add data\n",
    "        for dset_name in dict_dset[uuid].keys():\n",
    "            uuid_grp.create_dataset(dset_name, data=dict_dset[uuid][dset_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841e591d",
   "metadata": {},
   "source": [
    "# Open all recordings, calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113bc2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_attrs = dict()\n",
    "dict_data = dict()\n",
    "\n",
    "for root, dirs, files in os.walk(dset_folder):\n",
    "    for file in files:\n",
    "        fpath = os.path.join(root, file)\n",
    "        assert os.path.exists(fpath)\n",
    "        with h5py.File(fpath, \"r\") as hf:\n",
    "            dict_current_attrs = dict()\n",
    "            for key in hf.attrs.keys():\n",
    "                dict_current_attrs[key] = hf.attrs[key]\n",
    "            uuid = dict_current_attrs[\"uuid\"]\n",
    "            dict_attrs[uuid] = dict_current_attrs\n",
    "            \n",
    "            dict_current_data = dict()\n",
    "            for key in hf.keys():\n",
    "                dict_current_data[key] = hf[key][:]\n",
    "            dict_data[uuid] = dict_current_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a02e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00463c9",
   "metadata": {},
   "source": [
    "### Calculate pre-stim, post stim metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a53e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_uuids = []\n",
    "mouse_ids = []\n",
    "window_types = []\n",
    "exp_types = []\n",
    "segment_types = []\n",
    "segment_lengths = []\n",
    "totdists = []\n",
    "totdists_abs = []\n",
    "runnings = []\n",
    "totdists_norm = []\n",
    "totdists_abs_norm = []\n",
    "\n",
    "#df_stats[\"totdist_abs_norm\"] = 10000*df_stats[\"totdist_abs\"]/df_stats[\"segment_length\"]  # for totdist_abs, can use 4500 as length\n",
    "\n",
    "for uuid in dict_attrs.keys():\n",
    "    mouse_id = dict_attrs[uuid][\"mouse_id\"]\n",
    "    win_type = dict_attrs[uuid][\"win_type\"]\n",
    "    exp_type = dict_attrs[uuid][\"exp_type\"]\n",
    "    \n",
    "    # separate loco data into pre-stim and post-stim\n",
    "    dict_pre = dict()\n",
    "    dict_post = dict()\n",
    "    t_stim = dict_attrs[uuid][\"stim_start\"]\n",
    "    t_poststim = dict_attrs[uuid][\"poststim_start\"] \n",
    "    t_data = dict_data[uuid][\"lv_t_s\"]\n",
    "    \n",
    "    i_pre = t_data < t_stim\n",
    "    pre_length = np.sum(i_pre)\n",
    "    \n",
    "    # create matching post segment\n",
    "    i_first_post = np.argmax(t_data >= t_poststim)\n",
    "    i_post = np.zeros(t_data.shape)\n",
    "    i_post[i_first_post:i_first_post+pre_length] = 1\n",
    "    i_post = np.bool_(i_post)\n",
    "    post_length = np.sum(i_post)\n",
    "    if not pre_length == post_length:\n",
    "        print(pre_length)\n",
    "        print(post_length)\n",
    "    #i_post = np.logical_and(t_data >= t_poststim, t_data < )\n",
    "    \n",
    "    post_length = pre_length\n",
    "    for key in dict_data[uuid].keys():\n",
    "        if \"lv_\" in key:\n",
    "            data_arr = dict_data[uuid][key]\n",
    "            dict_pre[key] = data_arr[i_pre]\n",
    "            dict_post[key] = data_arr[i_post]\n",
    "    # calculate values\n",
    "    totdist_pre = dict_pre[\"lv_totdist\"][-1] - dict_pre[\"lv_totdist\"][0]\n",
    "    totdist_post = dict_post[\"lv_totdist\"][-1] - dict_post[\"lv_totdist\"][0]\n",
    "    \n",
    "    totdist_abs_pre = dict_pre[\"lv_totdist_abs\"][-1] - dict_pre[\"lv_totdist_abs\"][0]\n",
    "    totdist_abs_post = dict_post[\"lv_totdist_abs\"][-1] - dict_post[\"lv_totdist_abs\"][0]\n",
    "    \n",
    "    running_pre = np.sum(dict_pre[\"lv_running\"])*4500/pre_length  # normalize to other data\n",
    "    running_post = np.sum(dict_pre[\"lv_running\"])*4500/post_length\n",
    "    \n",
    "    totdist_norm_pre = 10000*totdist_pre/4500.\n",
    "    totdist_norm_post = 10000*totdist_post/4500.\n",
    "    \n",
    "    totdist_abs_norm_pre = 10000*totdist_abs_pre/4500.\n",
    "    totdist_abs_norm_post = 10000*totdist_abs_post/4500.\n",
    "    \n",
    "    # add pre\n",
    "    event_uuids.append(uuid)\n",
    "    mouse_ids.append(mouse_id)\n",
    "    window_types.append(win_type)\n",
    "    exp_types.append(exp_type)\n",
    "    segment_types.append(\"baseline\")  # \"post-stimulation\"\n",
    "    segment_lengths.append(4500)\n",
    "    totdists.append(totdist_pre)\n",
    "    totdists_abs.append(totdist_abs_pre)\n",
    "    runnings.append(running_pre)\n",
    "    totdists_norm.append(totdist_norm_pre)\n",
    "    totdists_abs_norm.append(totdist_abs_norm_pre)    \n",
    "    # add post\n",
    "    event_uuids.append(uuid)\n",
    "    mouse_ids.append(mouse_id)\n",
    "    window_types.append(win_type)\n",
    "    exp_types.append(exp_type)\n",
    "    segment_types.append(\"post-stimulation\")\n",
    "    segment_lengths.append(4500)\n",
    "    totdists.append(totdist_post)\n",
    "    totdists_abs.append(totdist_abs_post)\n",
    "    runnings.append(running_post)\n",
    "    totdists_norm.append(totdist_norm_post)\n",
    "    totdists_abs_norm.append(totdist_abs_norm_post)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1cbe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.DataFrame(data={\"event_uuid\": event_uuids, \"mouse_id\": mouse_ids, \"window_type\": window_types, \"exp_type\": exp_types, \"segment_type\": segment_types,\n",
    "                  \"segment_length\": segment_lengths, \"totdist\": totdists, \"totdist_abs\": totdists_abs, \"running\": runnings, \"totdist_norm\": totdists_norm, \"totdist_abs_norm\": totdists_abs_norm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351a798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.to_excel(f\"D:\\PhD\\Data\\\\bilat_loco_metrics_{get_datetime_for_fname()}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dd2c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6650183f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
