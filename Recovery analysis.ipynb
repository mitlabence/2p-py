{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recovery analysis\n",
    "(TMEV+ChR2 datasets combined)\n",
    "1. time to recovery\n",
    "2. Sz-bl, SD-bl amplitudes\n",
    "3. baseline-trough fluorescence difference\n",
    "4. peak-trough time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use recovery time point to set upper limit of trough index\n",
    "# TODO: brightest spot right now is found as absolute maximum. Seems good enough, but maybe a more robust method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_considered = 5  # x% darkest/brightest of complete trace to consider\n",
    "extreme_group_size = 15  # this many of the darkest/brightest pixels to consider (earliest darkest percent_considered% pixels)\n",
    "n_trough_frames = 5000  # simple method to set upper limit of window where to look for darkest point.\n",
    "peak_window_length = 300  # consider the first n frames when looking for peak\n",
    "imaging_freq = 15.  # approx, in hertz\n",
    "n_frames_before_am_start_nc = 200  # number of frames to consider additionally before aftermath begin for NC traces to look for trough. Necessary because optical end of seizure segment is \"darkest point\", so am category might just miss trough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [i for i in range(5000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window-related parameters\n",
    "window_width_s = 10\n",
    "window_step_s = 5\n",
    "imaging_frequency = 15. # in Hz\n",
    "n_frames_before_nc = 200  # include 200 frames just before aftermath for NC recordings  \n",
    "n_frames_before_ca1 = 0\n",
    "n_windows_post_darkest = 300 #40 # dataset consists of bl, darkest point, and this many windows post darkest point\n",
    "\n",
    "default_bl_center_ca1 = -75 # 4925 when 5000 bl frames\n",
    "default_bl_center_nc = -975  # 4025 when 5000 bl frames\n",
    "\n",
    "window_width_frames = int(window_width_s*imaging_frequency)\n",
    "window_step_frames = int(window_step_s*imaging_frequency)\n",
    "\n",
    "half_window_width_frames = window_width_frames//2\n",
    "\n",
    "recovery_ratio = 0.95  # reach x % of baseline to be considered recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_types_mapping = {\"CA1\" : \"CA1\", \"Cx\" : \"NC\"}  # replace Cx with NC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dsets = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figs = False\n",
    "save_as_eps = False\n",
    "save_as_pdf = True\n",
    "if save_as_eps:\n",
    "    output_format = \".eps\"\n",
    "elif save_as_pdf:\n",
    "    output_format=\".pdf\"\n",
    "else:\n",
    "    output_format = \".jpg\"\n",
    "if save_figs:\n",
    "    print(output_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries, set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auto-reload modules (used to develop functions outside this notebook)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import labrotation.file_handling as fh\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "import datadoc_util\n",
    "import h5py\n",
    "import numpy as np\n",
    "from math import floor\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(font_scale=2)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr2_fpath = fh.open_file(\"Open ChR2 assembled traces h5 file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmev_fpath = fh.open_file(\"Open TMEV assembled traces h5 file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_dict = dict()\n",
    "if not os.path.exists(\"./.env\"):\n",
    "    print(\".env does not exist\")\n",
    "else:\n",
    "    with open(\"./.env\", \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            l = line.rstrip().split(\"=\")\n",
    "            env_dict[l[0]] = l[1]\n",
    "print(env_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"DATA_DOCU_FOLDER\" in env_dict.keys():\n",
    "    docu_folder = env_dict[\"DATA_DOCU_FOLDER\"]\n",
    "else:\n",
    "    docu_folder = fh.open_dir(\"Choose folder containing folders for each mouse!\")\n",
    "print(f\"Selected folder:\\n\\t{docu_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"documentation\" in os.listdir(docu_folder):\n",
    "    mouse_folder = os.path.join(docu_folder, \"documentation\")\n",
    "else:\n",
    "    mouse_folder = docu_folder\n",
    "mouse_names = os.listdir(mouse_folder)\n",
    "print(f\"Mice detected:\")\n",
    "for mouse in mouse_names:\n",
    "    print(f\"\\t{mouse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datetime_for_fname():\n",
    "    now = datetime.now()\n",
    "    return f\"{now.year:04d}{now.month:02d}{now.day:02d}-{now.hour:02d}{now.minute:02d}{now.second:02d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = env_dict[\"DOWNLOADS_FOLDER\"]\n",
    "print(f\"Output files will be saved to {output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddoc = datadoc_util.DataDocumentation(docu_folder)\n",
    "ddoc.loadDataDoc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_mean_fluo = {} # uuid: [mean_fluo], cut to aftermath only!\n",
    "dict_bl_fluo = {}  # baseline (until segment_type_break_points[1]) \n",
    "dict_mid_fluo = {}  # rest of trace: sz or stim+sz\n",
    "dict_meta = {}  # uuid: {\"exp_type\": exp_type, \"mouse_id\": mouse_id, \"session_uuids\": [session_uuids]}\n",
    "\n",
    "dict_excluded = {}  # uuid: {\"exp_type\": exp_type, \"mouse_id\": mouse_id, \"win_type\": window_type, \"session_uuids\": [session_uuids]}\n",
    "\n",
    "dict_segment_break_points = {}  # uuid: (i_begin_mid, i_begin_am). bl: [:i_begin_mid], mid: [i_begin_mid:i_begin_am], am: [i_begin_am:]\n",
    "\n",
    "# Load traces. Set start time to appearance of first SD wave. TODO: maybe last SD wave must be used?\n",
    "for fpath in [tmev_fpath, chr2_fpath]:\n",
    "    with h5py.File(fpath, \"r\") as hf:\n",
    "        for event_uuid in hf.keys():\n",
    "            win_type = win_types_mapping[hf[event_uuid].attrs[\"window_type\"]]\n",
    "            assert \"session_uuids\" in hf[event_uuid].attrs\n",
    "            mouse_id = hf[event_uuid].attrs[\"mouse_id\"]\n",
    "            # for TMEV, traces were stitched together from multiple recordings, so uuid is not in data documentation. \n",
    "            # But the individual session uuids are stored in attributes (both for ChR2 and TMEV data)\n",
    "            session_uuids = hf[event_uuid].attrs[\"session_uuids\"]\n",
    "            exp_type = ddoc.getExperimentTypeForUuid(session_uuids[0])\n",
    "            mean_fluo = np.array(hf[event_uuid][\"mean_fluo\"])\n",
    "            if exp_type == \"tmev\":\n",
    "                # as TMEV traces are stitched together, it is difficult to use data documentation.\n",
    "                # But segment_type_break_points attribute contains bl, sz, am begin frames.\n",
    "                # am (aftermath) is defined as visual appearance of first SD wave. Can take this as beginning\n",
    "                segment_type_break_points = hf[event_uuid].attrs[\"segment_type_break_points\"]\n",
    "                assert len(segment_type_break_points) == 3  # make sure only bl, sz, am points are in list\n",
    "                i_begin_am = segment_type_break_points[2]\n",
    "                i_begin_mid = segment_type_break_points[1]  # one frame past end of baseline, i.e. begin of middle section (sz)\n",
    "                if win_type == \"NC\":  # NC seizures end abruptly, manual segmentation tries to set \"reaching darkest point\" as end of Sz. This means trough might be missed in original \"aftermath\" category.\n",
    "                    i_begin_am -= n_frames_before_am_start_nc\n",
    "                    assert i_begin_am > 0\n",
    "            elif exp_type in [\"chr2_sd\", \"chr2_szsd\"]:\n",
    "                assert session_uuids[0] == event_uuid\n",
    "                df_segments = ddoc.getSegmentsForUUID(event_uuid)\n",
    "                # set first frame of first SD appearance as beginning\n",
    "                i_begin_am = df_segments[df_segments[\"interval_type\"] == \"sd_wave\"].frame_begin.min() - 1  # 1-indexing to 0-indexing conversion\n",
    "                i_begin_mid = df_segments[df_segments[\"interval_type\"] == \"stimulation\"].frame_begin.min() - 1\n",
    "            else:\n",
    "                continue  # do not add chr2_ctl recordings to dataset \n",
    "            if not np.isnan(i_begin_am):\n",
    "                bl_fluo = mean_fluo[:i_begin_mid].copy()\n",
    "                mid_fluo = mean_fluo[i_begin_mid:i_begin_am].copy()\n",
    "                mean_fluo = mean_fluo[i_begin_am:]\n",
    "\n",
    "                dict_segment_break_points[event_uuid] = (i_begin_mid, i_begin_am)\n",
    "\n",
    "                dict_bl_fluo[event_uuid] = bl_fluo\n",
    "                dict_mean_fluo[event_uuid] = mean_fluo\n",
    "                dict_mid_fluo[event_uuid] = mid_fluo\n",
    "                dict_meta[event_uuid] = {\"exp_type\": exp_type, \"mouse_id\": mouse_id, \"win_type\": win_type, \"session_uuids\": session_uuids}\n",
    "            else:\n",
    "                dict_excluded[event_uuid] = {\"exp_type\": exp_type, \"mouse_id\": mouse_id, \"win_type\": win_type, \"session_uuids\": session_uuids}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble recovery dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define window-related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window(i_center, trace) -> np.array:\n",
    "    \"\"\"Given i_center and the global parameter half_window_width_frames, try to return a window centered around i_center, \n",
    "    and with inclusive borders at i_center - half_window_width_frames, i_center + half_window_width_frames. Might return a \n",
    "    smaller window [0, i_center + half_window_width_frames], or [i_center - half_window_width_frames, len(trace) - 1] if the \n",
    "    boundaries are outside the shape of trace.\n",
    "    Parameters\n",
    "    ----------\n",
    "    i_center : _type_\n",
    "        _description_\n",
    "    trace : _type_\n",
    "        _description_\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        _description_\n",
    "    \"\"\"\n",
    "    if i_center > len(trace):\n",
    "        warnings.warn(f\"Trying to access window with center {i_center}, but only {len(trace)} frames\")\n",
    "        return np.array([])\n",
    "    if i_center + half_window_width_frames > len(trace):\n",
    "        warnings.warn(f\"Part of window out of bounds: {i_center} + HW {half_window_width_frames} > {len(trace)}\")\n",
    "        right_limit = len(trace)\n",
    "    else:\n",
    "        right_limit = i_center + half_window_width_frames + 1  # right limit is exclusive\n",
    "    if i_center - half_window_width_frames < 0:\n",
    "        warnings.warn(f\"Part of window out of bounds: {i_center} - HW {half_window_width_frames} < 0\")\n",
    "        left_limit = 0\n",
    "    else:\n",
    "        left_limit = i_center - half_window_width_frames\n",
    "    return trace[left_limit : right_limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_for_window(trace_window):\n",
    "    lowest_indices = np.argsort(trace_window)[:int(percent_considered/100.*len(trace_window))]\n",
    "    lowest_values = trace_window[lowest_indices]\n",
    "    return np.median(lowest_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find baseline windows, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: manually correct f0442bebcd1a4291a8d0559eb47df08e\n",
    "#dict_uuid_manual_bl_center = {\"aa66ae0470a14eb08e9bcadedc34ef64\": 4250, \"c7b29d28248e493eab02288b85e3adee\": 4000,  \"7b9c17d8a1b0416daf65621680848b6a\": 4050, \"9e75d7135137444492d104c461ddcaac\": 4700, \"d158cd12ad77489a827dab1173a933f9\": 4500, \"a39ed3a880c54f798eff250911f1c92f\" : 4500, \"4e2310d2dde845b0908519b7196080e8\" : 4500, \"f0442bebcd1a4291a8d0559eb47df08e\": 4500, \"2251bba132cf45fa839d3214d1651392\": 3700, \"cd3c1e0e3c284a89891d2e4d9a7461f4\": 3500}\n",
    "dict_uuid_manual_bl_center = {\"aa66ae0470a14eb08e9bcadedc34ef64\": -750, \"c7b29d28248e493eab02288b85e3adee\": -1000,  \"7b9c17d8a1b0416daf65621680848b6a\": -950, \"9e75d7135137444492d104c461ddcaac\": -300, \"d158cd12ad77489a827dab1173a933f9\": -500, \"a39ed3a880c54f798eff250911f1c92f\" : -500, \"4e2310d2dde845b0908519b7196080e8\" : -500, \"f0442bebcd1a4291a8d0559eb47df08e\": -500, \"2251bba132cf45fa839d3214d1651392\": -1300, \"cd3c1e0e3c284a89891d2e4d9a7461f4\": -1500}\n",
    "\n",
    "# uuid: (i_bl, bl_metric), i_bl is the center of the window\n",
    "dict_bl_values = {}\n",
    "\n",
    "for uuid in dict_meta.keys():  # uuid: {\"exp_type\": exp_type, \"mouse_id\": mouse_id, \"session_uuids\": [session_uuids]}\n",
    "    exp_type = dict_meta[uuid][\"exp_type\"]\n",
    "    win_type = dict_meta[uuid][\"win_type\"]\n",
    "    # check if manually corrected. If not, check if TMEV or not. If TMEV, use default_bl_center_ca1/default_bl_center_nc\n",
    "    # if ChR2, can use a window right before stim\n",
    "    bl_trace = dict_bl_fluo[uuid]\n",
    "    if uuid in dict_uuid_manual_bl_center:\n",
    "        i_bl = dict_uuid_manual_bl_center[uuid]\n",
    "    elif exp_type == \"tmev\":\n",
    "        if win_type == \"CA1\":\n",
    "            i_bl = default_bl_center_ca1\n",
    "        elif win_type == \"NC\":\n",
    "            i_bl = default_bl_center_nc\n",
    "    elif exp_type in [\"chr2_sd\", \"chr2_szsd\"]:\n",
    "        # take a window just before stim\n",
    "        i_bl = len(bl_trace) - half_window_width_frames - 1\n",
    "    if i_bl < 0:\n",
    "        i_bl = len(bl_trace) + i_bl\n",
    "    bl_win = get_window(i_bl, bl_trace)\n",
    "    bl_metric = get_metric_for_window(bl_win)\n",
    "    dict_bl_values[uuid] = (i_bl, bl_metric)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event_uuid in dict_meta:\n",
    "    if dict_meta[event_uuid][\"exp_type\"] == \"tmev\" and dict_meta[event_uuid][\"win_type\"] == \"CA1\":\n",
    "        print(f\"{event_uuid}:\\t{dict_bl_values[event_uuid][0]}\\t{len(dict_bl_fluo[event_uuid])}\\t{dict_bl_values[event_uuid][1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event_uuid in dict_meta:\n",
    "    if dict_meta[event_uuid][\"exp_type\"] == \"tmev\" and dict_meta[event_uuid][\"win_type\"] == \"NC\":\n",
    "        print(f\"{event_uuid}:\\t{dict_bl_values[event_uuid][0]}\\t{len(dict_bl_fluo[event_uuid])}\\t{dict_bl_values[event_uuid][1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find peak, trough (darkest point), recovery position\n",
    "Same method as Baseline recovery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aftermath:\n",
    "# TMEV - appearance of first SD. This could also be taken above\n",
    "# ChR2 - if SD present, then appearance of first SD. Else: directly after stim (ctl).\n",
    "\n",
    "dict_peak_trough = {}  # uuid: (i_peak, i_trough, peak_amplitude, trough_amplitude)\n",
    "\n",
    "for event_uuid in dict_mean_fluo.keys(): \n",
    "    exp_type = dict_meta[event_uuid][\"exp_type\"]\n",
    "    win_type = dict_meta[event_uuid][\"exp_type\"]\n",
    "\n",
    "    # traces already cut to \"aftermath\" (plus few extra frames)\n",
    "    complete_trace = dict_mean_fluo[event_uuid]\n",
    "    \n",
    "    # get 5% darkest points of aftermath\n",
    "    sorted_indices = np.argsort(complete_trace)  # this cut should not influence the index\n",
    "    \n",
    "    # get brightest frame\n",
    "    # old method, uses same percentages and median as darkest frame. Did not work well\n",
    "    #i_brightest_group = np.flip(sorted_indices)[:int(percent_considered/100.*len(sorted_indices))]\n",
    "    #i_brightest = int(floor(np.median(np.sort(i_brightest_group)[:extreme_group_size])))\n",
    "    sorted_beginning = np.argsort(complete_trace[:peak_window_length])\n",
    "    i_brightest = sorted_beginning[-1]\n",
    "\n",
    "    cut_trace = complete_trace[i_brightest:i_brightest+n_trough_frames]\n",
    "    # use reduced window to look for trough\n",
    "    sorted_indices_cut = np.argsort(cut_trace)\n",
    "    i_darkest_group = sorted_indices_cut[:int(percent_considered/100.*len(complete_trace))]  # still take n percent of aftermath, not cut trace!\n",
    "    # get single coordinate for darkest part\n",
    "    # find darkest <percent_considered>%, take earliest <extreme_group_size> of them, get median frame index of these, round down to integer frame\n",
    "    i_darkest_cut = int(floor(np.median(np.sort(i_darkest_group)[:extreme_group_size])))\n",
    "\n",
    "    i_darkest = i_darkest_cut + i_brightest  # bring it back to original frame indices\n",
    "    assert i_darkest > i_brightest\n",
    "\n",
    "    y_brightest = complete_trace[i_brightest]\n",
    "    #y_darkest = complete_trace[i_darkest]  # TODO: get window value instead?\n",
    "    y_darkest = get_window(i_darkest, complete_trace)\n",
    "    y_darkest = get_metric_for_window(y_darkest)\n",
    "    print(f\"{event_uuid}\\t{i_brightest}\\t{i_darkest}\\t{y_brightest}\\t{y_darkest}\")\n",
    "    assert y_brightest > y_darkest\n",
    "\n",
    "    # find time of half maximum\n",
    "    y_half = (y_brightest + y_darkest)/2.  # bl + (peak - bl)/2\n",
    "    i_half = np.argmax(complete_trace[i_brightest:] <= y_half)\n",
    "    i_half += i_brightest\n",
    "    assert i_brightest < i_half\n",
    "    assert i_darkest > i_half\n",
    "    dict_peak_trough[event_uuid] = (i_brightest, i_darkest, i_half, y_brightest, y_darkest, y_half)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with trough, find time window where metric shows recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_recovery = {}  # event_uuid: (i_recovery, y_recovery, did_recover)\n",
    "dict_windows = {}  # event_uuid: [y_bl_window, y_darkest_window, y_post_darkest1, y_post_darkest2, ..., y_recovery_window]\n",
    "\n",
    "for event_uuid in dict_mean_fluo:\n",
    "    trace = dict_mean_fluo[event_uuid]\n",
    "    i_trough = dict_peak_trough[event_uuid][1]\n",
    "    y_windows = []\n",
    "    did_recover = False  # assume recovery will be found\n",
    "    # add y_bl\n",
    "    y_bl = dict_bl_values[event_uuid][1]\n",
    "    y_windows.append(y_bl)  \n",
    "    \n",
    "    # add trough window to windows list\n",
    "    i_current = i_trough\n",
    "    current_win = get_window(i_current, trace)  # start with metric at trough\n",
    "    y_current = get_metric_for_window(current_win)\n",
    "    y_windows.append(y_current)\n",
    "    \n",
    "    # move on to next window just after trough to start looking for recovery (FIXME: in some cases, already trough is > 95% of bl! by definition we demand recovery to happen after the trough?)\n",
    "    i_current += window_step_frames\n",
    "    current_win = get_window(i_current, trace)\n",
    "    while len(current_win) >= window_width_frames:  # stop algorithm upon reaching end of recording\n",
    "        y_current = get_metric_for_window(current_win)\n",
    "        y_windows.append(y_current)\n",
    "        if y_current >= recovery_ratio*y_bl:  # recovery reached\n",
    "            did_recover = True\n",
    "            break\n",
    "        else:  # move to next window\n",
    "            i_current += window_step_frames\n",
    "            current_win = get_window(i_current, trace)\n",
    "    dict_windows[event_uuid] = y_windows\n",
    "    dict_recovery[event_uuid] = (i_current, y_current, did_recover)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event_uuid in dict_peak_trough:\n",
    "    if event_uuid not in dict_segment_break_points:\n",
    "        print(event_uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (raw) columns: event_uuid, mouse_id, experiment_type, peak_time, trough_time, peak_amplitude, trough_amplitude \n",
    "df_recovery = pd.DataFrame.from_dict(dict_peak_trough, \"index\", columns=[\"i_peak\", \"i_trough\", \"i_half\", \"y_peak\", \"y_trough\", \"y_half\"]).reset_index()\n",
    "# replace column name \"index\" with \"event_uuid\"\n",
    "df_recovery[\"event_uuid\"] = df_recovery[\"index\"] \n",
    "df_recovery = df_recovery.drop(columns=[\"index\"])\n",
    "df_recovery[\"exp_type\"] = df_recovery.apply(lambda row: dict_meta[row.event_uuid][\"exp_type\"], axis=1)\n",
    "df_recovery[\"mouse_id\"] = df_recovery.apply(lambda row: dict_meta[row.event_uuid][\"mouse_id\"], axis=1)\n",
    "\n",
    "df_recovery[\"y_bl\"] = df_recovery.apply(lambda row: dict_bl_values[row[\"event_uuid\"]][1], axis=1)\n",
    "df_recovery[\"i_bl\"] = df_recovery.apply(lambda row: dict_bl_values[row[\"event_uuid\"]][0], axis=1)\n",
    "\n",
    "# peak minus trough difference in amplitude\n",
    "df_recovery[\"dy_bl_trough\"] = df_recovery[\"y_bl\"] - df_recovery[\"y_trough\"]\n",
    "# peak-trough time difference, s\n",
    "df_recovery[\"dt_peak_trough\"] = df_recovery[\"i_trough\"]/imaging_freq - df_recovery[\"i_peak\"]/imaging_freq\n",
    "# peak to half amplitude time difference, s\n",
    "df_recovery[\"dt_peak_trough_FWHM\"] = df_recovery[\"i_trough\"]/imaging_freq - df_recovery[\"i_peak\"]/imaging_freq\n",
    "\n",
    "df_recovery[\"i_recovery\"] = df_recovery.apply(lambda row: dict_recovery[row[\"event_uuid\"]][0], axis=1)\n",
    "df_recovery[\"y_recovery\"] = df_recovery.apply(lambda row: dict_recovery[row[\"event_uuid\"]][1], axis=1)\n",
    "df_recovery[\"did_recover\"] = df_recovery.apply(lambda row: dict_recovery[row[\"event_uuid\"]][2], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df_recovery[\"dt_trough_recovery\"] = df_recovery[\"i_recovery\"]/imaging_freq - df_recovery[\"i_trough\"]/imaging_freq\n",
    "df_recovery[\"dt_peak_recovery\"] = df_recovery[\"i_recovery\"]/imaging_freq - df_recovery[\"i_peak\"]/imaging_freq\n",
    "\n",
    "\n",
    "# move i_xy to whole trace indexing frame of reference\n",
    "df_recovery[\"i_recovery_whole\"] = df_recovery.apply(lambda row: row[\"i_recovery\"] + dict_segment_break_points[row[\"event_uuid\"]][1], axis=1)\n",
    "df_recovery[\"i_peak_whole\"] = df_recovery.apply(lambda row: row[\"i_peak\"] + dict_segment_break_points[row[\"event_uuid\"]][1], axis=1)\n",
    "df_recovery[\"i_trough_whole\"] = df_recovery.apply(lambda row: row[\"i_trough\"] + dict_segment_break_points[row[\"event_uuid\"]][1], axis=1)\n",
    "\n",
    "\n",
    "# final columns: event_uuid, mouse_id, exp_type, y_bl, y_peak, y_trough, y_recovery, dy_trough_peak, dt_peak_trough, dt_peak_trough_FWHM, dt_trough_recovery, dt_peak_recovery, did_recover\n",
    "df_recovery = df_recovery[[\"event_uuid\", \"mouse_id\", \"exp_type\",  \"y_bl\", \"y_peak\", \"y_trough\", \"y_recovery\", \"dy_bl_trough\", \"dt_peak_trough\", \"dt_peak_trough_FWHM\", \"dt_trough_recovery\", \"dt_peak_recovery\", \"did_recover\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: where did_recover is False, need to implement linear extrapolation.\n",
    "df_recovery[[\"event_uuid\", \"dt_trough_recovery\", \"did_recover\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot detected peak/trough values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 42))\n",
    "\n",
    "AMPLITUDE = 100.0\n",
    "offset = 0.0\n",
    "color_dict = {\"tmev\": \"green\", \"chr2_sd\": \"red\", \"chr2_szsd\": \"blue\"}\n",
    "def normalize_trace(trace):\n",
    "    min_trace = np.min(trace)\n",
    "    max_trace = np.max(trace)\n",
    "    return AMPLITUDE*(trace - min_trace)/(max_trace - min_trace)\n",
    "\n",
    "for event_uuid in df_recovery.sort_values(by=[\"exp_type\", \"event_uuid\"]).event_uuid:\n",
    "    exp_type = dict_meta[event_uuid][\"exp_type\"]\n",
    "    plt.plot(normalize_trace(dict_mean_fluo[event_uuid]) + offset, color=color_dict[exp_type], label=exp_type)\n",
    "    # plot brightest point\n",
    "    plt.vlines(x=dict_peak_trough[event_uuid][0], ymin=offset, ymax = offset+AMPLITUDE )  # peak\n",
    "    plt.vlines(x=dict_peak_trough[event_uuid][1], ymin=offset, ymax = offset+AMPLITUDE, color=\"black\" )  # trough\n",
    "    plt.vlines(x=dict_peak_trough[event_uuid][2], ymin=offset, ymax = offset+AMPLITUDE, color=\"orange\" )  # half max\n",
    "\n",
    "\n",
    "    offset += AMPLITUDE\n",
    "\n",
    "\n",
    "plt.legend(color_dict)\n",
    "ax = plt.gca()\n",
    "leg = ax.get_legend()\n",
    "# manually set colors of legend... reading the dict colors does not work for some reason\n",
    "leg.legendHandles[0].set_color(\"green\")\n",
    "leg.legendHandles[1].set_color('red')\n",
    "leg.legendHandles[2].set_color('blue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot peak-trough time per experiment type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 10))\n",
    "g = sns.boxplot(data=df_recovery, y=\"delta_t_FWHM\", hue=\"exp_type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 10))\n",
    "g = sns.histplot(data=df_recovery, x=\"delta_t\", hue=\"exp_type\",multiple=\"stack\", bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peak-trough amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 10))\n",
    "g = sns.boxplot(data=df_recovery, y=\"delta_amp\", hue=\"exp_type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: smoothing and first derivative for minimum?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
