{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recovery analysis\n",
    "(TMEV+ChR2 datasets combined)\n",
    "1. time to recovery\n",
    "2. Sz-bl, SD-bl amplitudes\n",
    "3. baseline-trough fluorescence difference\n",
    "4. peak-trough time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_considered = 5  # x% darkest/brightest of complete trace to consider\n",
    "extreme_group_size = 15  # this many of the darkest/brightest pixels to consider (earliest darkest percent_considered% pixels)\n",
    "n_trough_frames = 5000  # simple method to set upper limit of window where to look for darkest point.\n",
    "peak_window_length = 300  # consider the first n frames when looking for peak\n",
    "imaging_freq = 15.  # approx, in hertz\n",
    "# The following was replaced with a manually set dict:\n",
    "#n_frames_before_am_start_nc = 0#200  # number of frames to consider additionally before aftermath begin for NC traces to look for trough. Necessary because optical end of seizure segment is \"darkest point\", so am category might just miss trough.\n",
    "# WARNING: reducing n_frames_before_am_start_nc might result in the trough falling before the peak (in NC traces). Weird, but it happens... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window-related parameters\n",
    "window_width_s = 10\n",
    "window_step_s = 5\n",
    "imaging_frequency = 15. # in Hz\n",
    "n_frames_before_nc = 200  # include 200 frames just before aftermath for NC recordings  \n",
    "n_frames_before_ca1 = 0\n",
    "n_windows_post_darkest = 300 #40 # dataset consists of bl, darkest point, and this many windows post darkest point\n",
    "\n",
    "default_bl_center_ca1 = -75 # 4925 when 5000 bl frames\n",
    "default_bl_center_nc = -975  # 4025 when 5000 bl frames\n",
    "\n",
    "window_width_frames = int(window_width_s*imaging_frequency)\n",
    "window_step_frames = int(window_step_s*imaging_frequency)\n",
    "\n",
    "half_window_width_frames = window_width_frames//2\n",
    "\n",
    "recovery_ratio = 0.95  # reach x % of baseline to be considered recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the aftermath categories for NC are arbitrary: they begin around the trough, but the starting frame was visually set. \n",
    "# Manual corrections to the dataset: ()\n",
    "n_frames_before_am_start_nc = {\n",
    "    \"2251bba132cf45fa839d3214d1651392\": 125,\n",
    "    \"4dea78a01bf5408092f498032d67d84e\": 205,\n",
    "    \"54c31c3151944cfd86043932d3a19b9a\": 60,\n",
    "    \"5cfb012d47f14303a40680d2b333336a\": 125,\n",
    "    \"7753b03a2a554cccaab42f1c0458d742\": 70,\n",
    "    \"cd3c1e0e3c284a89891d2e4d9a7461f4\": 192,\n",
    "    \"f481149fa8694621be6116cb84ae2d3c\": 115,\n",
    "    \"f5ccb81a34bb434482e2498bfdf88784\": 58,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_types_mapping = {\"CA1\" : \"CA1\", \"Cx\" : \"NC\"}  # replace Cx with NC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dsets = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figs = False\n",
    "save_as_eps = False\n",
    "save_as_pdf = True\n",
    "if save_as_eps:\n",
    "    output_format = \".eps\"\n",
    "elif save_as_pdf:\n",
    "    output_format=\".pdf\"\n",
    "else:\n",
    "    output_format = \".jpg\"\n",
    "if save_figs:\n",
    "    print(output_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries, set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auto-reload modules (used to develop functions outside this notebook)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import labrotation.file_handling as fh\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "import datadoc_util\n",
    "import h5py\n",
    "import numpy as np\n",
    "from math import floor, ceil\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from numpy.polynomial.polynomial import Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(font_scale=2)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr2_fpath = fh.open_file(\"Open ChR2 assembled traces h5 file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmev_fpath = fh.open_file(\"Open TMEV assembled traces h5 file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_dict = dict()\n",
    "if not os.path.exists(\"./.env\"):\n",
    "    print(\".env does not exist\")\n",
    "else:\n",
    "    with open(\"./.env\", \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            l = line.rstrip().split(\"=\")\n",
    "            env_dict[l[0]] = l[1]\n",
    "print(env_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"DATA_DOCU_FOLDER\" in env_dict.keys():\n",
    "    docu_folder = env_dict[\"DATA_DOCU_FOLDER\"]\n",
    "else:\n",
    "    docu_folder = fh.open_dir(\"Choose folder containing folders for each mouse!\")\n",
    "print(f\"Selected folder:\\n\\t{docu_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"documentation\" in os.listdir(docu_folder):\n",
    "    mouse_folder = os.path.join(docu_folder, \"documentation\")\n",
    "else:\n",
    "    mouse_folder = docu_folder\n",
    "mouse_names = os.listdir(mouse_folder)\n",
    "print(f\"Mice detected:\")\n",
    "for mouse in mouse_names:\n",
    "    print(f\"\\t{mouse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datetime_for_fname():\n",
    "    now = datetime.now()\n",
    "    return f\"{now.year:04d}{now.month:02d}{now.day:02d}-{now.hour:02d}{now.minute:02d}{now.second:02d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = env_dict[\"DOWNLOADS_FOLDER\"]\n",
    "print(f\"Output files will be saved to {output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddoc = datadoc_util.DataDocumentation(docu_folder)\n",
    "ddoc.loadDataDoc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_mean_fluo = {} # uuid: [mean_fluo], cut to aftermath (+ extra frames) only!\n",
    "dict_bl_fluo = {}  # baseline (until segment_type_break_points[1]) \n",
    "dict_mid_fluo = {}  # rest of trace: sz or stim+sz\n",
    "# to get complete trace for event_uuid, np.concatenate([dict_bl_fluo[event_uuid], dict_mid_fluo[event_uuid], dict_mean_fluo[event_uuid]])\n",
    "dict_meta = {}  # uuid: {\"exp_type\": exp_type, \"mouse_id\": mouse_id, \"session_uuids\": [session_uuids], \"segment_type_break_points\": [segment_type_break_points]}\n",
    "\n",
    "dict_excluded = {}  # uuid: {\"exp_type\": exp_type, \"mouse_id\": mouse_id, \"win_type\": window_type, \"session_uuids\": [session_uuids]}\n",
    "\n",
    "dict_segment_break_points = {}  # uuid: (i_begin_mid, i_begin_am). bl: [:i_begin_mid], mid: [i_begin_mid:i_begin_am], am: [i_begin_am:]\n",
    "\n",
    "# Load traces. Set start time to appearance of first SD wave. TODO: maybe last SD wave must be used?\n",
    "for fpath in [tmev_fpath, chr2_fpath]:\n",
    "    with h5py.File(fpath, \"r\") as hf:\n",
    "        for event_uuid in hf.keys():\n",
    "            win_type = win_types_mapping[hf[event_uuid].attrs[\"window_type\"]]\n",
    "            assert \"session_uuids\" in hf[event_uuid].attrs\n",
    "            mouse_id = hf[event_uuid].attrs[\"mouse_id\"]\n",
    "            # for TMEV, traces were stitched together from multiple recordings, so uuid is not in data documentation. \n",
    "            # But the individual session uuids are stored in attributes (both for ChR2 and TMEV data)\n",
    "            session_uuids = hf[event_uuid].attrs[\"session_uuids\"]\n",
    "            exp_type = ddoc.getExperimentTypeForUuid(session_uuids[0])\n",
    "            mean_fluo = np.array(hf[event_uuid][\"mean_fluo\"])\n",
    "            segment_type_break_points = hf[event_uuid].attrs[\"segment_type_break_points\"]\n",
    "            if exp_type == \"tmev\":\n",
    "                # as TMEV traces are stitched together, it is difficult to use data documentation.\n",
    "                # But segment_type_break_points attribute contains bl, sz, am begin frames.\n",
    "                # am (aftermath) is defined as visual appearance of first SD wave. Can take this as beginning\n",
    "                assert len(segment_type_break_points) == 3  # make sure only bl, sz, am points are in list\n",
    "                i_begin_am = segment_type_break_points[2]\n",
    "                i_begin_mid = segment_type_break_points[1]  # one frame past end of baseline, i.e. begin of middle section (sz)\n",
    "                if win_type == \"NC\":  # NC seizures end abruptly, manual segmentation tries to set \"reaching darkest point\" as end of Sz. This means trough might be missed in original \"aftermath\" category.\n",
    "                    i_begin_am -= n_frames_before_am_start_nc[event_uuid]\n",
    "                    assert i_begin_am > 0\n",
    "            elif exp_type in [\"chr2_sd\", \"chr2_szsd\"]:\n",
    "                assert session_uuids[0] == event_uuid\n",
    "                df_segments = ddoc.getSegmentsForUUID(event_uuid)\n",
    "                # set first frame of first SD appearance as beginning\n",
    "                i_begin_am = df_segments[df_segments[\"interval_type\"] == \"sd_wave\"].frame_begin.min() - 1  # 1-indexing to 0-indexing conversion\n",
    "                i_begin_mid = df_segments[df_segments[\"interval_type\"] == \"stimulation\"].frame_begin.min() - 1\n",
    "            else:\n",
    "                continue  # do not add chr2_ctl recordings to dataset \n",
    "            if not np.isnan(i_begin_am):\n",
    "                bl_fluo = mean_fluo[:i_begin_mid].copy()\n",
    "                mid_fluo = mean_fluo[i_begin_mid:i_begin_am].copy()\n",
    "                if not len(mid_fluo) > 0:\n",
    "                    print(f\"{i_begin_mid} - {i_begin_am}\")\n",
    "                mean_fluo = mean_fluo[i_begin_am:]\n",
    "\n",
    "                dict_segment_break_points[event_uuid] = (i_begin_mid, i_begin_am)\n",
    "\n",
    "                dict_bl_fluo[event_uuid] = bl_fluo\n",
    "                dict_mean_fluo[event_uuid] = mean_fluo\n",
    "                dict_mid_fluo[event_uuid] = mid_fluo\n",
    "                dict_meta[event_uuid] = {\"exp_type\": exp_type, \"mouse_id\": mouse_id, \"win_type\": win_type, \"session_uuids\": session_uuids, \"segment_type_break_points\": segment_type_break_points}\n",
    "            else:\n",
    "                dict_excluded[event_uuid] = {\"exp_type\": exp_type, \"mouse_id\": mouse_id, \"win_type\": win_type, \"session_uuids\": session_uuids, \"segment_type_break_points\": segment_type_break_points}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_am_begin_nc = False\n",
    "if check_am_begin_nc:\n",
    "    fig = plt.figure(figsize=(20, 42))\n",
    "    AMPLITUDE = 100\n",
    "    offset = 0.0\n",
    "    for event_uuid in dict_meta:\n",
    "        exp_type = dict_meta[event_uuid][\"exp_type\"]\n",
    "        win_type = dict_meta[event_uuid][\"win_type\"]\n",
    "        if exp_type == \"tmev\" and win_type == \"NC\":\n",
    "            full_trace = np.concatenate([dict_bl_fluo[event_uuid][-200:], dict_mid_fluo[event_uuid], dict_mean_fluo[event_uuid]])\n",
    "            norm_trace = AMPLITUDE*(full_trace - np.min(full_trace))/(np.max(full_trace) - np.min(full_trace))\n",
    "            plt.plot(norm_trace + offset)\n",
    "            plt.vlines(x=len(full_trace) - len(dict_mean_fluo[event_uuid]), ymin=offset, ymax = offset+1.1*AMPLITUDE, color=\"black\")\n",
    "            plt.text(800, offset+0.3*AMPLITUDE, event_uuid, fontdict={\"fontsize\":40})\n",
    "            offset += 1.1*AMPLITUDE\n",
    "            print(event_uuid)\n",
    "            \n",
    "\n",
    "    plt.xlim((0, 1500))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble recovery dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define window-related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window(i_center, trace) -> np.array:\n",
    "    \"\"\"Given i_center and the global parameter half_window_width_frames, try to return a window centered around i_center, \n",
    "    and with inclusive borders at i_center - half_window_width_frames, i_center + half_window_width_frames. Might return a \n",
    "    smaller window [0, i_center + half_window_width_frames], or [i_center - half_window_width_frames, len(trace) - 1] if the \n",
    "    boundaries are outside the shape of trace.\n",
    "    Parameters\n",
    "    ----------\n",
    "    i_center : int\n",
    "        The index of center of the window in trace\n",
    "    trace : np.array\n",
    "        The trace to extract the window from\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        The window, a subarray of trace\n",
    "    \"\"\"\n",
    "    if i_center > len(trace):\n",
    "        warnings.warn(f\"Trying to access window with center {i_center}, but only {len(trace)} frames\")\n",
    "        return np.array([])\n",
    "    if i_center + half_window_width_frames > len(trace):\n",
    "        warnings.warn(f\"Part of window out of bounds: {i_center} + HW {half_window_width_frames} > {len(trace)}\")\n",
    "        right_limit = len(trace)\n",
    "    else:\n",
    "        right_limit = i_center + half_window_width_frames + 1  # right limit is exclusive\n",
    "    if i_center - half_window_width_frames < 0:\n",
    "        warnings.warn(f\"Part of window out of bounds: {i_center} - HW {half_window_width_frames} < 0\")\n",
    "        left_limit = 0\n",
    "    else:\n",
    "        left_limit = i_center - half_window_width_frames\n",
    "    return trace[left_limit : right_limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SD_WINDOW_WIDTH_FRAMES = 450  # 30 s window beginning with \"am\" segment to look for amplitude of SD\n",
    "def get_window_for_event_type(event_uuid, event_type=\"sz\"):\n",
    "    \"\"\"Given the event uuid and the event type (sz, sd) to look for, return np.array() of the corresponding window in\n",
    "    the whole trace of the original hdf5 data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    event_uuid : str\n",
    "        the event_uuid of the trace (the name of the hdf5 group)\n",
    "    event_type : str\n",
    "        \"sd\" or \"sz\". The event for which the window to be returned: end of bl/stim until beginning of first SD if \"sz\", else a fixed\n",
    "        30s window starting with the appearance of the first SD wave.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        The window (empty array if event_type does not exist for the recording type)\n",
    "    \"\"\"\n",
    "    exp_type = dict_meta[event_uuid][\"exp_type\"]\n",
    "    win_type = dict_meta[event_uuid][\"win_type\"]\n",
    "    complete_trace = np.concatenate([dict_bl_fluo[event_uuid], dict_mid_fluo[event_uuid], dict_mean_fluo[event_uuid]])\n",
    "\n",
    "    if exp_type == \"tmev\":\n",
    "        break_points = dict_meta[event_uuid][\"segment_type_break_points\"]  # [bl_begin, sz_begin, SD_begin]\n",
    "        if event_type == \"sz\":  # am begins with appearance of first SD wave -> if sz, get time second and third indices\n",
    "            return complete_trace[break_points[1]:break_points[2]]\n",
    "        elif event_type == \"sd\" and win_type != \"NC\":  # prove me wrong, but no SD in NC. :)  \n",
    "            return complete_trace[break_points[2]: break_points[2] + SD_WINDOW_WIDTH_FRAMES]\n",
    "    elif \"chr2\" in exp_type:\n",
    "        df_segments = ddoc.getSegmentsForUUID(event_uuid)  # sessions consist of one recording, so event_uuid = recording_uuid\n",
    "        if event_type == \"sz\" and exp_type == \"chr2_szsd\":\n",
    "            i_begin_sz = df_segments[df_segments[\"interval_type\"] == \"sz\"].frame_begin.iloc[0] - 1  # switch to 0-based indexing\n",
    "            i_end_sz = df_segments[df_segments[\"interval_type\"] == \"sz\"].frame_end.iloc[0]  # upper limit exclusive\n",
    "            return complete_trace[i_begin_sz: i_end_sz]\n",
    "        elif event_type == \"sd\" and \"sd\" in exp_type:\n",
    "            i_begin_sd = df_segments[df_segments[\"interval_type\"] == \"sd_wave\"].frame_begin.iloc[0] - 1  # switch to 0-based indexing\n",
    "            i_end_sd = i_begin_sd + SD_WINDOW_WIDTH_FRAMES\n",
    "            return complete_trace[i_begin_sd:i_end_sd]\n",
    "    warnings.warn(\"No window found!\")\n",
    "    return np.array([])\n",
    "\n",
    "def get_metric_for_window(trace_window):\n",
    "    \"\"\"Given a window, calculate the following metric: \n",
    "    1. Take percent_considered % of the lowest values within the window\n",
    "    2. Get the median value of the values found in step 1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trace_window : np.array\n",
    "        The window to calculate the metric for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The calculated metric \n",
    "    \"\"\"\n",
    "    lowest_indices = np.argsort(trace_window)[:int(percent_considered/100.*len(trace_window))]\n",
    "    lowest_values = trace_window[lowest_indices]\n",
    "    return np.median(lowest_values)\n",
    "\n",
    "def get_peak_metric(trace_window):\n",
    "    \"\"\"Given a trace window, calculate the mean of top 5% values. Intended use: SD and Sz amplitudes.\n",
    "    Parameters\n",
    "    ----------\n",
    "    trace_window : np.array\n",
    "        The window to calculate the metric for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The calculated metric\n",
    "    \"\"\"\n",
    "\n",
    "    mean_top_5p = np.flip(np.sort(trace_window))[:int(0.05*len(trace_window))].mean()  # take mean of highest 5% of sz values\n",
    "    return mean_top_5p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find baseline windows, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_uuid_manual_bl_center = {\"aa66ae0470a14eb08e9bcadedc34ef64\": 4250, \"c7b29d28248e493eab02288b85e3adee\": 4000,  \"7b9c17d8a1b0416daf65621680848b6a\": 4050, \"9e75d7135137444492d104c461ddcaac\": 4700, \"d158cd12ad77489a827dab1173a933f9\": 4500, \"a39ed3a880c54f798eff250911f1c92f\" : 4500, \"4e2310d2dde845b0908519b7196080e8\" : 4500, \"f0442bebcd1a4291a8d0559eb47df08e\": 4500, \"2251bba132cf45fa839d3214d1651392\": 3700, \"cd3c1e0e3c284a89891d2e4d9a7461f4\": 3500}\n",
    "# fix the dict to work with arbitrary length baseline instead of length 5000 only.\n",
    "dict_uuid_manual_bl_center = {\"aa66ae0470a14eb08e9bcadedc34ef64\": -750, \"c7b29d28248e493eab02288b85e3adee\": -1000,  \"7b9c17d8a1b0416daf65621680848b6a\": -950, \"9e75d7135137444492d104c461ddcaac\": -300, \"d158cd12ad77489a827dab1173a933f9\": -500, \"a39ed3a880c54f798eff250911f1c92f\" : -500, \"4e2310d2dde845b0908519b7196080e8\" : -500, \"f0442bebcd1a4291a8d0559eb47df08e\": -500, \"2251bba132cf45fa839d3214d1651392\": -1300, \"cd3c1e0e3c284a89891d2e4d9a7461f4\": -1500}\n",
    "\n",
    "# uuid: (i_bl, bl_metric), i_bl is the center of the window\n",
    "dict_bl_values = {}\n",
    "\n",
    "for uuid in dict_meta.keys():  # uuid: {\"exp_type\": exp_type, \"mouse_id\": mouse_id, \"session_uuids\": [session_uuids]}\n",
    "    exp_type = dict_meta[uuid][\"exp_type\"]\n",
    "    win_type = dict_meta[uuid][\"win_type\"]\n",
    "    # check if manually corrected. If not, check if TMEV or not. If TMEV, use default_bl_center_ca1/default_bl_center_nc\n",
    "    # if ChR2, can use a window right before stim\n",
    "    bl_trace = dict_bl_fluo[uuid]\n",
    "    if uuid in dict_uuid_manual_bl_center:\n",
    "        i_bl = dict_uuid_manual_bl_center[uuid]\n",
    "    elif exp_type == \"tmev\":\n",
    "        if win_type == \"CA1\":\n",
    "            i_bl = default_bl_center_ca1\n",
    "        elif win_type == \"NC\":\n",
    "            i_bl = default_bl_center_nc\n",
    "    elif exp_type in [\"chr2_sd\", \"chr2_szsd\"]:\n",
    "        # take a window just before stim\n",
    "        i_bl = len(bl_trace) - half_window_width_frames - 1\n",
    "    if i_bl < 0:\n",
    "        i_bl = len(bl_trace) + i_bl\n",
    "    bl_win = get_window(i_bl, bl_trace)\n",
    "    bl_metric = get_metric_for_window(bl_win)\n",
    "    dict_bl_values[uuid] = (i_bl, bl_metric)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find significant time points and corresponding values\n",
    "Sz amplitude (if exists), peak, trough (darkest point), recovery position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aftermath:\n",
    "# TMEV - appearance of first SD. This could also be taken above\n",
    "# ChR2 - if SD present, then appearance of first SD. Else: directly after stim (ctl).\n",
    "\n",
    "dict_significant_tpoints = {}  # uuid: (i_sd_peak, i_trough, i_fwhm, peak_amplitude, trough_amplitude, fwhm_amplitude=peak_amplitude/2)\n",
    "\n",
    "for event_uuid in dict_mean_fluo.keys(): \n",
    "    exp_type = dict_meta[event_uuid][\"exp_type\"]\n",
    "    win_type = dict_meta[event_uuid][\"win_type\"]\n",
    "\n",
    "    # traces already cut to \"aftermath\" (plus few extra frames)\n",
    "    complete_trace = dict_mean_fluo[event_uuid]\n",
    "    \n",
    "    # get 5% darkest points of aftermath\n",
    "    sorted_indices = np.argsort(complete_trace)  # this cut should not influence the index\n",
    "    \n",
    "    # get brightest frame\n",
    "    # old method, uses same percentages and median as darkest frame. Did not work well\n",
    "    #i_brightest_group = np.flip(sorted_indices)[:int(percent_considered/100.*len(sorted_indices))]\n",
    "    #i_brightest = int(floor(np.median(np.sort(i_brightest_group)[:extreme_group_size])))\n",
    "    sorted_beginning = np.argsort(complete_trace[:peak_window_length])\n",
    "    i_brightest = sorted_beginning[-1]  # this is supposed to be SD amplitude. Later, set it to np.nan if there is no SD.\n",
    "    i_sd_peak = i_brightest \n",
    "\n",
    "    cut_trace = complete_trace[i_brightest:i_brightest+n_trough_frames]\n",
    "    # use reduced window to look for trough\n",
    "    sorted_indices_cut = np.argsort(cut_trace)\n",
    "    i_darkest_group = sorted_indices_cut[:int(percent_considered/100.*len(complete_trace))]  # still take n percent of aftermath, not cut trace!\n",
    "    # get single coordinate for darkest part\n",
    "    # find darkest <percent_considered>%, take earliest <extreme_group_size> of them, get median frame index of these, round down to integer frame\n",
    "    i_darkest_cut = int(floor(np.median(np.sort(i_darkest_group)[:extreme_group_size])))\n",
    "\n",
    "    i_darkest = i_darkest_cut + i_brightest  # bring it back to original frame indices\n",
    "\n",
    "\n",
    "    # get Sz and SD amplitude metrics\n",
    "    #y_brightest = complete_trace[i_brightest]\n",
    "    # TODO: originally, i_brightest was the index of maximum brightness. In Baseline recovery, the SD amplitude uses different approach\n",
    "    #       as implemented below. Need to remove i_brightest and old y_brightest = complete_trace[i_brightest]! (brightest is SD peak)\n",
    "    sd_window = get_window_for_event_type(event_uuid, \"sd\")\n",
    "    y_sd_peak = get_peak_metric(sd_window)\n",
    "    if len(sd_window) == 0:\n",
    "        i_sd_peak = np.nan \n",
    "\n",
    "    sz_window = get_window_for_event_type(event_uuid, \"sz\")\n",
    "    y_sz_peak = get_peak_metric(sz_window)\n",
    "\n",
    "\n",
    "    #y_darkest = complete_trace[i_darkest]  # TODO: get window value instead?\n",
    "    y_darkest = get_window(i_darkest, complete_trace)\n",
    "    y_darkest = get_metric_for_window(y_darkest)\n",
    "\n",
    "    # find time of half maximum\n",
    "    if not np.isnan(i_sd_peak):\n",
    "        y_half = (y_sd_peak + y_darkest)/2.  # bl + (peak - bl)/2\n",
    "    if not np.isnan(i_sd_peak):\n",
    "        i_half = np.argmax(complete_trace[i_brightest:] <= y_half)\n",
    "        i_half += i_sd_peak\n",
    "    else:\n",
    "        i_half = np.nan\n",
    "        y_half = np.nan\n",
    "\n",
    "    #print()\n",
    "    #print(i_darkest)\n",
    "    #print(i_half)\n",
    "    #assert i_brightest < i_half\n",
    "    #assert i_darkest > i_half\n",
    "\n",
    "    if win_type == \"NC\":  # no SD in NC windows... Correct this logic if I'm wrong :)\n",
    "        print(f\"{event_uuid} window type is NC. No SD = no peak.\")\n",
    "        i_brightest = -1\n",
    "        y_brightest = np.nan\n",
    "    dict_significant_tpoints[event_uuid] = (i_sd_peak, i_darkest, i_half, y_sd_peak, y_darkest, y_half)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find seizure amplitude (if exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: this should be outdated, as sz and sd amplpitudes are added to dict_significant_tpoints above\n",
    "# uuid: (i_mid_max, y_mid_max) where i_mid_max is the frame index of the mid segment (dict_mid_fluo). \n",
    "#  Stim is ignored when finding the max.\n",
    "dict_sz_amps = {}  # contains absolute amplitude, not compared to baseline!\n",
    "\n",
    "for event_uuid in dict_mean_fluo.keys(): \n",
    "    exp_type = dict_meta[event_uuid][\"exp_type\"]\n",
    "    win_type = dict_meta[event_uuid][\"exp_type\"]\n",
    "    if exp_type in [\"chr2_szsd\", \"tmev\"]:  # only consider recordings where seizure occurs\n",
    "        mid_trace = dict_mid_fluo[event_uuid]\n",
    "        if exp_type == \"chr2_szsd\":  # ignore stim frames\n",
    "            df_segments = ddoc.getSegmentsForUUID(event_uuid)\n",
    "            assert \"sz\" in df_segments.interval_type.unique()  # make sure sz actually occurred\n",
    "            # get number of stim frames to ignore. mid section begins with stim frames.\n",
    "            i_begin_stim = df_segments[df_segments[\"interval_type\"] == \"stimulation\"].frame_begin.iloc[0]  # inclusive\n",
    "            i_end_stim = df_segments[df_segments[\"interval_type\"] == \"stimulation\"].frame_end.iloc[0]  # inclusive\n",
    "            n_stim_frames = i_end_stim - i_begin_stim + 1\n",
    "            mid_trace = mid_trace[n_stim_frames:]\n",
    "        else:  # if tmev, make sure sz segment exists\n",
    "            session_uuids = dict_meta[event_uuid][\"session_uuids\"]\n",
    "            sz_present = False\n",
    "            for session_uuid in session_uuids:\n",
    "                df_segments = ddoc.getSegmentsForUUID(session_uuid)\n",
    "                if \"sz\" in df_segments.interval_type.unique():\n",
    "                    sz_present = True\n",
    "                    break\n",
    "            if not sz_present:\n",
    "                dict_sz_amps[event_uuid] = np.nan\n",
    "                print(event_uuid)\n",
    "                continue\n",
    "        dict_sz_amps[event_uuid] = np.max(mid_trace)\n",
    "    else:  # no seizure in experiment\n",
    "        dict_sz_amps[event_uuid] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with trough, find time window where metric shows recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows_from(i_begin_center, trace):\n",
    "    \"\"\"Given a trace and the 0-based index of the center of a first window, return the indices and the corresponding window metrics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    i_begin_center : int\n",
    "        The 0-based index of the first window center to include\n",
    "    trace : np.array\n",
    "        The trace\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple(np.array, np.array)\n",
    "        A tuple with two arrays: at location 0, the 0-based window centers and at location 1, the corresponding window metrics.\n",
    "    \"\"\"\n",
    "    i_center_current = i_begin_center\n",
    "    x_vals = []\n",
    "    y_vals = []\n",
    "    while i_center_current < len(trace) - half_window_width_frames:  # stop algorithm upon reaching end of recording\n",
    "        current_win = get_window(i_center_current, trace)\n",
    "        y_current = get_metric_for_window(current_win)\n",
    "        x_vals.append(i_center_current)\n",
    "        y_vals.append(y_current)\n",
    "\n",
    "        i_center_current += window_step_frames\n",
    "    return (np.array(x_vals), np.array(y_vals))\n",
    "\n",
    "def try_extrapolate_recovery(x_vals, y_vals, y_expol):\n",
    "    \"\"\"Given the x values x_vals and the corresponding y-values y_vals, try to find the x value corresponding to y_expol, \n",
    "    based on linear extrapolation. This algorithm is specialized on finding recovery time point, so a line with positive slope is sought.\n",
    "    If this cannot be found, a large time point is returned.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_vals : np.array\n",
    "        The x values (in the notebook, intended use is frames inb 0-based indexing, the center of windows)\n",
    "    y_vals : np.array\n",
    "        The y values (intended use is the metrics of the windows specified by x_vals)\n",
    "    y_expol : int (or scalar, same as y_vals.dtype)\n",
    "        The y value to extrapolate to. (intended use case: recovery_ratio*y_baseline)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The found extrapolated time point, in same units as x_vals. (intended use case: the frame index where 95% of baseline is reached)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        line_fit_coeffs = Polynomial.fit(x_vals, y_vals, deg=1).convert().coef  # linear fit starting with point after darkest time point\n",
    "        # the coefficients [a, b] from y= a + b*x.\n",
    "        if line_fit_coeffs[1] <= 0:  # Check if b is non-positive -> No recovery possible\n",
    "            #x_recovery_single_ca1.append(np.inf)\n",
    "            return 25000  # set a very late recovery. TODO: come up with better value! np.inf messes up statistics...\n",
    "    except np.linalg.LinAlgError as e:\n",
    "        print(f\"Could not extrapolate. Returning last window center as extrapolation time point...\")\n",
    "        return x_vals[-1]\n",
    "    else:  # b>0 -> line is ascending, i.e. there will be a recovery time\n",
    "        # find inverse function. We know y = a + b*x, need to have x = c + d*y, where y = <threshold>*baseline (threshold=0.95)\n",
    "        # inverse is x = -a/b + (1/b)*y = a_inv + b_inv*y\n",
    "        a_inv = -line_fit_coeffs[0]/line_fit_coeffs[1]\n",
    "        b_inv = 1/line_fit_coeffs[1]\n",
    "        x_recovery = a_inv + b_inv*recovery_ratio*y_expol\n",
    "        x_recovery = ceil(x_recovery)\n",
    "        return x_recovery\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_recovery = {}  # event_uuid: (i_recovery, y_recovery, did_recover)\n",
    "dict_windows = {}  # event_uuid: [y_bl_window, y_darkest_window, y_post_darkest1, y_post_darkest2, ..., y_recovery_window]\n",
    "\n",
    "for event_uuid in dict_mean_fluo:\n",
    "    trace = dict_mean_fluo[event_uuid]\n",
    "    i_trough = dict_significant_tpoints[event_uuid][1]\n",
    "    \n",
    "    x_windows = []  # the 0-indexed window center coordinates [x_bl, x_trough, x_window1, ...]\n",
    "    y_windows = []  # the corresponding window metrics [y_bl, y_trough, y_window1, ...]\n",
    "    \n",
    "    did_recover = False  # assume recovery will be found\n",
    "    # add y_bl\n",
    "    i_bl = dict_bl_values[event_uuid][0]\n",
    "    y_bl = dict_bl_values[event_uuid][1]\n",
    "    x_windows.append(i_bl)\n",
    "    y_windows.append(y_bl)  \n",
    "    \n",
    "    # add trough window to windows list\n",
    "    i_current = i_trough\n",
    "    current_win = get_window(i_current, trace)  # start with metric at trough\n",
    "    y_current = get_metric_for_window(current_win)\n",
    "    y_windows.append(y_current)\n",
    "    x_windows.append(i_current)\n",
    "\n",
    "    # move on to next window just after trough to start looking for recovery (FIXME: in some cases, already trough is > 95% of bl! by definition we demand recovery to happen after the trough?)\n",
    "    i_current += window_step_frames\n",
    "    current_win = get_window(i_current, trace)\n",
    "    while len(current_win) >= window_width_frames:  # stop algorithm upon reaching end of recording\n",
    "        y_current = get_metric_for_window(current_win)\n",
    "        y_windows.append(y_current)\n",
    "        x_windows.append(i_current)\n",
    "        if y_current >= recovery_ratio*y_bl:  # recovery reached\n",
    "            did_recover = True\n",
    "            break\n",
    "        else:  # move to next window\n",
    "            i_current += window_step_frames\n",
    "            current_win = get_window(i_current, trace)\n",
    "    # if no recovery found within trace, try to extrapolate. Start with window after trough.\n",
    "    if not did_recover:\n",
    "        x_recovery = try_extrapolate_recovery(x_windows[2:], y_windows[2:], recovery_ratio*y_bl)\n",
    "        y_recovery = recovery_ratio*y_bl\n",
    "    else:\n",
    "        x_recovery = i_current\n",
    "        y_recovery = y_current\n",
    "\n",
    "    dict_windows[event_uuid] = y_windows\n",
    "\n",
    "    dict_recovery[event_uuid] = (x_recovery, y_recovery, did_recover)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (raw) columns: event_uuid, mouse_id, experiment_type, peak_time, trough_time, peak_amplitude, trough_amplitude \n",
    "df_recovery = pd.DataFrame.from_dict(dict_significant_tpoints, \"index\", columns=[\"i_peak\", \"i_trough\", \"i_half\", \"y_peak\", \"y_trough\", \"y_half\"]).reset_index()\n",
    "# replace column name \"index\" with \"event_uuid\"\n",
    "df_recovery[\"event_uuid\"] = df_recovery[\"index\"] \n",
    "df_recovery = df_recovery.drop(columns=[\"index\"])\n",
    "df_recovery[\"exp_type\"] = df_recovery.apply(lambda row: dict_meta[row.event_uuid][\"exp_type\"], axis=1)\n",
    "df_recovery[\"mouse_id\"] = df_recovery.apply(lambda row: dict_meta[row.event_uuid][\"mouse_id\"], axis=1)\n",
    "df_recovery[\"win_type\"] = df_recovery.apply(lambda row: dict_meta[row.event_uuid][\"win_type\"], axis=1)\n",
    "\n",
    "df_recovery[\"y_bl\"] = df_recovery.apply(lambda row: dict_bl_values[row[\"event_uuid\"]][1], axis=1)\n",
    "df_recovery[\"i_bl\"] = df_recovery.apply(lambda row: dict_bl_values[row[\"event_uuid\"]][0], axis=1)\n",
    "\n",
    "# peak minus trough difference in amplitude\n",
    "df_recovery[\"dy_bl_trough\"] = df_recovery[\"y_bl\"] - df_recovery[\"y_trough\"]\n",
    "# peak-trough time difference, s\n",
    "df_recovery[\"dt_peak_trough\"] = df_recovery[\"i_trough\"]/imaging_freq - df_recovery[\"i_peak\"]/imaging_freq\n",
    "# peak to half amplitude time difference, s\n",
    "df_recovery[\"dt_peak_trough_FWHM\"] = df_recovery[\"i_trough\"]/imaging_freq - df_recovery[\"i_peak\"]/imaging_freq\n",
    "\n",
    "df_recovery[\"i_recovery\"] = df_recovery.apply(lambda row: dict_recovery[row[\"event_uuid\"]][0], axis=1)\n",
    "df_recovery[\"y_recovery\"] = df_recovery.apply(lambda row: dict_recovery[row[\"event_uuid\"]][1], axis=1)\n",
    "df_recovery[\"did_recover\"] = df_recovery.apply(lambda row: dict_recovery[row[\"event_uuid\"]][2], axis=1)\n",
    "df_recovery[\"extrapolated\"] = ~df_recovery[\"did_recover\"]\n",
    "\n",
    "\n",
    "df_recovery[\"dt_trough_recovery\"] = df_recovery[\"i_recovery\"]/imaging_freq - df_recovery[\"i_trough\"]/imaging_freq\n",
    "df_recovery[\"dt_peak_recovery\"] = df_recovery[\"i_recovery\"]/imaging_freq - df_recovery[\"i_peak\"]/imaging_freq\n",
    "\n",
    "\n",
    "# move i_xy to whole trace indexing frame of reference\n",
    "df_recovery[\"i_recovery_whole\"] = df_recovery.apply(lambda row: row[\"i_recovery\"] + dict_segment_break_points[row[\"event_uuid\"]][1], axis=1)\n",
    "df_recovery[\"i_peak_whole\"] = df_recovery.apply(lambda row: row[\"i_peak\"] + dict_segment_break_points[row[\"event_uuid\"]][1], axis=1)\n",
    "df_recovery[\"i_trough_whole\"] = df_recovery.apply(lambda row: row[\"i_trough\"] + dict_segment_break_points[row[\"event_uuid\"]][1], axis=1)\n",
    "\n",
    "df_recovery[\"y_sz_max\"] = df_recovery.apply(lambda row: dict_sz_amps[row[\"event_uuid\"]], axis=1)\n",
    "\n",
    "df_recovery[\"dy_bl_sz\"] = df_recovery[\"y_sz_max\"] - df_recovery[\"y_bl\"]\n",
    "df_recovery[\"dy_bl_sd\"] = df_recovery[\"y_peak\"] - df_recovery[\"y_bl\"]  # peak of aftermath is the largest SD amplitude\n",
    "# final columns: event_uuid, mouse_id, exp_type, y_bl, y_peak, y_trough, y_recovery, dy_trough_peak, dt_peak_trough, dt_peak_trough_FWHM, dt_trough_recovery, dt_peak_recovery, did_recover\n",
    "df_recovery = df_recovery[[\"event_uuid\", \"mouse_id\", \"exp_type\", \"win_type\",  \"y_bl\", \"y_sz_max\", \"y_peak\", \"y_trough\", \"y_recovery\", \"dy_bl_sz\", \"dy_bl_sd\", \"dy_bl_trough\", \"dt_peak_trough\", \"dt_peak_trough_FWHM\", \"dt_trough_recovery\", \"dt_peak_recovery\", \"extrapolated\"]].sort_values(by=[\"exp_type\", \"win_type\", \"event_uuid\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Recovery time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following dataframe contains recovery time and experiment metadata:\n",
    "# if did_recover is false, extrapolation was used\n",
    "df_recovery_time = df_recovery[[ \"mouse_id\", \"exp_type\", \"win_type\", \"event_uuid\", \"dt_trough_recovery\", \"extrapolated\"]].sort_values(by=[\"exp_type\", \"win_type\", \"mouse_id\"])\n",
    "df_recovery_time = df_recovery_time.rename(columns={\"win_type\": \"window_type\", \"dt_trough_recovery\": \"t_recovery_s\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_dsets:\n",
    "    fpath_recovery = os.path.join(output_folder, f\"recovery_times_{get_datetime_for_fname()}.xlsx\")\n",
    "    df_recovery_time.to_excel(fpath_recovery, index=False)\n",
    "    print(f\"Saved file to {fpath_recovery}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bl-Sz, Bl-SD amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amplitudes = df_recovery[[\"mouse_id\", \"event_uuid\", \"exp_type\", \"win_type\", \"dy_bl_sz\", \"dy_bl_sd\"]].sort_values(by=[\"exp_type\", \"win_type\", \"mouse_id\"])\n",
    "df_amplitudes = df_amplitudes.rename(columns={\"dy_bl_sz\": \"Sz-bl\", \"dy_bl_sd\": \"SD-bl\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_dsets:\n",
    "    fpath_amplitudes = os.path.join(output_folder, f\"sz_sd_amplitudes_{get_datetime_for_fname()}.xlsx\")\n",
    "    df_amplitudes.to_excel(fpath_amplitudes, index=False)\n",
    "    print(f\"Saved file to {fpath_amplitudes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Baseline-trough difference amplitude (amount of depression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bl_darkest = df_recovery[[ \"mouse_id\", \"event_uuid\", \"exp_type\", \"win_type\", \"y_bl\", \"y_trough\", \"dy_bl_trough\", \"extrapolated\"]].sort_values(by=[\"exp_type\", \"win_type\", \"mouse_id\"])\n",
    "df_bl_darkest = df_bl_darkest.rename(columns={\"y_bl\": \"baseline\", \"y_trough\": \"darkest_postictal\", \"dy_bl_trough\": \"bl-darkest\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_dsets:\n",
    "    fpath_bl_darkest = os.path.join(output_folder, f\"bl-to-darkest-point_{get_datetime_for_fname()}.xlsx\")\n",
    "    df_bl_darkest.to_excel(fpath_bl_darkest, index=False)\n",
    "    print(f\"Saved file to {fpath_bl_darkest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Peak FWHM - trough time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_peak_trough_fwhm = df_recovery[[\"event_uuid\", \"mouse_id\", \"exp_type\", \"win_type\", \"dt_peak_trough_FWHM\", \"extrapolated\"]].sort_values(by=[\"exp_type\", \"win_type\", \"mouse_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_dsets:\n",
    "    fpath_peak_trough = os.path.join(output_folder, f\"peak_trough_fwhm_{get_datetime_for_fname()}.xlsx\")\n",
    "    df_peak_trough_fwhm.to_excel(fpath_peak_trough, index=False)\n",
    "    print(f\"Saved file to {fpath_peak_trough}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot detected peak/trough values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 42))\n",
    "\n",
    "AMPLITUDE = 100.0\n",
    "offset = 0.0\n",
    "dict_colors = {\"tmev\": \"green\", \"chr2_sd\": \"red\", \"chr2_szsd\": \"blue\"}\n",
    "dict_significant_frames_colors = {\"peak\": \"limegreen\", \"half_max\": \"darkgreen\", \"trough\": \"black\", \"recovery\":\"green\" }\n",
    "\n",
    "def normalize_trace(trace):\n",
    "    min_trace = np.min(trace)\n",
    "    max_trace = np.max(trace)\n",
    "    return AMPLITUDE*(trace - min_trace)/(max_trace - min_trace)\n",
    "\n",
    "for event_uuid in df_recovery.sort_values(by=[\"exp_type\", \"mouse_id\"]).event_uuid:\n",
    "    exp_type = dict_meta[event_uuid][\"exp_type\"]\n",
    "    mouse_id = dict_meta[event_uuid][\"mouse_id\"]\n",
    "    trace_color = ddoc.getColorForMouseId(mouse_id)\n",
    "    # plot small part of bl, whole mid section, and whole aftermath\n",
    "    bl_trace = dict_bl_fluo[event_uuid][-200:]\n",
    "    n_cut_frames = len(dict_bl_fluo[event_uuid]) - len(bl_trace)\n",
    "\n",
    "    mid_trace = dict_mid_fluo[event_uuid]\n",
    "    am_trace = dict_mean_fluo[event_uuid][:5000]\n",
    "\n",
    "    i_shift = len(bl_trace) + len(mid_trace)  # shift indices for peak, trough, etc. to account for extra bl, mid frames \n",
    "\n",
    "    full_trace = np.concatenate([bl_trace, mid_trace, am_trace])\n",
    "    if exp_type in [\"chr2_sd\", \"chr2_szsd\"]:  # need to reduce stim amplitude to make sz, sd more visible\n",
    "        df_segments = ddoc.getSegmentsForUUID(event_uuid)\n",
    "        i_begin_stim = df_segments[df_segments[\"interval_type\"] == \"stimulation\"].frame_begin.iloc[0] - n_cut_frames - 1  # switch to 0-based indexing\n",
    "        i_end_stim = df_segments[df_segments[\"interval_type\"] == \"stimulation\"].frame_end.iloc[0] - n_cut_frames  # exclusive limit to numpy [a:b] indexing\n",
    "        full_trace[i_begin_stim:i_end_stim] = np.max(full_trace[i_end_stim:])  # set stim amplitude to maximum of signal to not lose details when scaling trace\n",
    "    plt.plot(normalize_trace(full_trace) + offset, color=trace_color, label=exp_type)\n",
    "    # plot significant time points\n",
    "    plt.vlines(x=dict_significant_tpoints[event_uuid][0] + i_shift, ymin=offset, ymax = offset+1.1*AMPLITUDE, color=dict_significant_frames_colors[\"peak\"])  # peak\n",
    "    plt.vlines(x=dict_significant_tpoints[event_uuid][2] + i_shift, ymin=offset, ymax = offset+1.1*AMPLITUDE, color=dict_significant_frames_colors[\"half_max\"] )  # half max\n",
    "    plt.vlines(x=dict_significant_tpoints[event_uuid][1] + i_shift, ymin=offset, ymax = offset+1.1*AMPLITUDE, color=dict_significant_frames_colors[\"trough\"] )  # trough\n",
    "    plt.vlines(x=dict_recovery[event_uuid][0] + i_shift, ymin=offset, ymax=offset+1.1*AMPLITUDE, color=dict_significant_frames_colors[\"recovery\"])\n",
    "\n",
    "\n",
    "    offset += AMPLITUDE\n",
    "\n",
    "\n",
    "plt.legend(dict_significant_frames_colors)\n",
    "ax = plt.gca()\n",
    "leg = ax.get_legend()\n",
    "# manually set colors of legend... reading the dict colors does not work for some reason\n",
    "leg.legendHandles[0].set_color(dict_significant_frames_colors[\"peak\"])\n",
    "leg.legendHandles[2].set_color(dict_significant_frames_colors[\"half_max\"])\n",
    "leg.legendHandles[1].set_color(dict_significant_frames_colors[\"trough\"])\n",
    "leg.legendHandles[3].set_color(dict_significant_frames_colors[\"recovery\"])\n",
    "\n",
    "\n",
    "plt.xlim((0, 7500))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event_uuid in dict_mean_fluo.keys():\n",
    "    y_peak = df_recovery[df_recovery[\"event_uuid\"] == event_uuid][\"y_peak\"].iloc[0] \n",
    "    full_trace = np.concatenate([dict_mean_fluo[event_uuid] ])\n",
    "    if not y_peak == np.max(full_trace):\n",
    "        print(f\"{dict_meta[event_uuid]['mouse_id']}: {y_peak}, {np.max(full_trace)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot peak-trough time per experiment type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 10))\n",
    "g = sns.boxplot(data=df_recovery, y=\"delta_t_FWHM\", hue=\"exp_type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 10))\n",
    "g = sns.histplot(data=df_recovery, x=\"delta_t\", hue=\"exp_type\",multiple=\"stack\", bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peak-trough amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 10))\n",
    "g = sns.boxplot(data=df_recovery, y=\"delta_amp\", hue=\"exp_type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: smoothing and first derivative for minimum?\n",
    "# TODO: implement all 3 analyses (recovery time, Sz/SD amplitudes, bl-trough amplitude difference = amount of depression)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
