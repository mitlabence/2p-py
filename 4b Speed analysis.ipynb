{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a799b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dsets = False\n",
    "\n",
    "save_figs = True  # set to True to save the figures created\n",
    "save_as_eps = False\n",
    "save_as_pdf = True\n",
    "if save_as_pdf:\n",
    "    file_format = \".pdf\"\n",
    "elif save_as_eps:\n",
    "    file_format = \".eps\"\n",
    "else:\n",
    "    file_format = \".jpg\"\n",
    "if save_figs:\n",
    "    print(f\"Going to save figures as {file_format} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b066a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auto-reload modules (used to develop functions outside this notebook)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ace3f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from nd2_to_caiman import np_arr_from_nd2\n",
    "import labrotation.file_handling as fh\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib as mpl\n",
    "from math import floor, ceil, sqrt, atan2, acos, pi, sin, cos\n",
    "from datetime import datetime\n",
    "import json\n",
    "from labrotation import json_util\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.stats import circmean, circstd  # for statistical testing on directionality\n",
    "import datadoc_util\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import multiprocess as mp  # multiprocessing does not work with IPython. Use fork instead.\n",
    "import os\n",
    "import random  # for surrogate algorithm\n",
    "from collections.abc import Iterable\n",
    "import math\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1454af",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams.update({'font.size': 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdcf5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"whitegrid\")\n",
    "color_palette = sns.color_palette(\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01514347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datetime_for_fname():\n",
    "    now = datetime.now()\n",
    "    return f\"{now.year:04d}{now.month:02d}{now.day:02d}-{now.hour:02d}{now.minute:02d}{now.second:02d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e180ebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_shape = (8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a59122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_dict = dict()\n",
    "if not os.path.exists(\"./.env\"):\n",
    "    print(\".env does not exist\")\n",
    "else:\n",
    "    with open(\"./.env\", \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            l = line.rstrip().split(\"=\")\n",
    "            env_dict[l[0]] = l[1]\n",
    "print(env_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909cb7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"DATA_DOCU_FOLDER\" in env_dict.keys():\n",
    "    data_docu_folder = env_dict[\"DATA_DOCU_FOLDER\"]\n",
    "else:\n",
    "    data_docu_folder = fh.open_dir(\"Open Data Documentation folder\")\n",
    "print(data_docu_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddoc = datadoc_util.DataDocumentation(data_docu_folder)\n",
    "ddoc.loadDataDoc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffde5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_colors = ddoc.getColorings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc7ddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_mouse_id_color = {row[\"mouse_id\"]: row[\"color\"] for i_row, row in df_colors.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f0e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = env_dict[\"DOWNLOADS_FOLDER\"]\n",
    "print(f\"Output files will be saved to {output_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edacb9a",
   "metadata": {},
   "source": [
    "## Open files and get uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1628baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id_uuid = ddoc.getIdUuid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4b806",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_folder = fh.open_dir(\"Open directory with analysis (grid) data for all mice!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f30ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_files_list = []\n",
    "for root, dirs, files in os.walk(analysis_folder):\n",
    "    for fname in files:\n",
    "        if \"_grid.h5\" in fname:\n",
    "            grid_files_list.append(os.path.join(root,fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27920495",
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid_dict = dict()\n",
    "exp_type_dict = dict()\n",
    "uuid_exp_type_dict = dict()\n",
    "for grid_fpath in grid_files_list:\n",
    "    # ..._grid.h5 -> ..._cnmf.hdf5\n",
    "    cnmf_fpath = os.path.join(os.path.split(grid_fpath)[0], os.path.split(grid_fpath)[-1][:-7] + \"cnmf.hdf5\")\n",
    "    with h5py.File(cnmf_fpath, 'r') as hf:\n",
    "        uuid = hf.attrs[\"uuid\"]\n",
    "        exp_type = ddoc.getExperimentTypeForUuid(uuid)\n",
    "        uuid_dict[grid_fpath] = hf.attrs[\"uuid\"]\n",
    "        exp_type_dict[grid_fpath] = exp_type\n",
    "        uuid_exp_type_dict[hf.attrs[\"uuid\"]] = exp_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f26fc6",
   "metadata": {},
   "source": [
    "## Combine all results into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83795c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_set = set()\n",
    "for fpath in grid_files_list:\n",
    "    df = pd.read_hdf(fpath)\n",
    "    for key in df.keys():\n",
    "        cols_set.add(key)\n",
    "cols_set.add(\"uuid\")\n",
    "cols_set.add(\"mouse_id\")\n",
    "cols_set.add(\"exp_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bb316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining empty dataframe does not work, as all data types will be object (except x, y, which will be proper integers)\n",
    "all_onsets_df = pd.read_hdf(grid_files_list[0])\n",
    "all_onsets_df[\"uuid\"] = uuid_dict[grid_files_list[0]]\n",
    "all_onsets_df[\"mouse_id\"] = df_id_uuid[df_id_uuid[\"uuid\"] == uuid_dict[grid_files_list[0]]][\"mouse_id\"].values[0]\n",
    "all_onsets_df[\"exp_type\"] = exp_type_dict[grid_files_list[0]]\n",
    "\n",
    "assert all_onsets_df[\"uuid\"].isna().sum() == 0\n",
    "for fpath in grid_files_list[1:]:\n",
    "    df = pd.read_hdf(fpath)\n",
    "    df[\"uuid\"] = uuid_dict[fpath]\n",
    "    df[\"mouse_id\"] = df_id_uuid[df_id_uuid[\"uuid\"] == uuid_dict[fpath]][\"mouse_id\"].values[0]\n",
    "    df[\"exp_type\"] = exp_type_dict[fpath]\n",
    "    \n",
    "    assert df[\"uuid\"].isna().sum() == 0\n",
    "    assert df[\"exp_type\"].isna().sum() == 0\n",
    "    \n",
    "    all_onsets_df = pd.concat([all_onsets_df, df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff75dc6",
   "metadata": {},
   "source": [
    "### Remove 5% most deviant onset from median for each category\n",
    "Wenzel 2017 (recruitment reliability) page 8, below fig 5 (not caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff7c804",
   "metadata": {},
   "source": [
    "### Make sure to have integer and float data types for the columns, and not object! (int16, int64, float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f4a496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for old files containing onset data, n_seizures was not present, as one of the last recordings processed contained 2. \n",
    "# As a result, most of i_sz values are NaN; these contain 1 sz. Otherwise 0, 1... are the seizure indices.\n",
    "if \"i_sz\" not in all_onsets_df:  # in case we did not need it, still have the column\n",
    "    all_onsets_df[\"i_sz\"] = np.nan\n",
    "all_onsets_df[\"i_sz\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c23a859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make seizures unique in uuid_extended\n",
    "def append_uuid(row):\n",
    "    if pd.isna(row['i_sz']):\n",
    "        return row['uuid']\n",
    "    elif row['i_sz'] >= 0:\n",
    "        return row['uuid'] + '_' + str(row[\"i_sz\"]+1)\n",
    "all_onsets_df['uuid_extended'] = all_onsets_df.apply(append_uuid, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e654db",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = all_onsets_df.groupby(\"mouse_id\")\n",
    "for group in g:\n",
    "    print(group[0])\n",
    "    g2 = group[1].groupby(\"uuid\")\n",
    "    for grp in g2:\n",
    "        print(\"\\t\" + grp[0])\n",
    "        print(\"\\t\" + str(len(grp[1])) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9d5842",
   "metadata": {},
   "source": [
    "`all_onsets_df` now contains each recording with seizure and/or SD each neuron. For each neuron, there is a value for onset of each SD and seizure wave (NaN for all neurons in a session if none occurred in the recording) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ccb97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_outliers(group, colname):\n",
    "    # calculate 5% highest deviation threshold\n",
    "    # get deviations\n",
    "    if colname not in group.keys():\n",
    "        print(group)\n",
    "        raise Exception\n",
    "\n",
    "    median_colname = group[colname].dropna().median()\n",
    "\n",
    "    deviations = np.abs(group[colname] - median_colname)\n",
    "    deviations_nonan = np.abs(group[colname].dropna() - median_colname)\n",
    "    if len(deviations_nonan) == 0:  # empty column (only NaN): skip outlier removal\n",
    "        return group\n",
    "    # sort in descending order\n",
    "    deviations_sorted_desc = np.flip(np.sort(deviations_nonan))\n",
    "    # get 5% threshold deviation value\n",
    "    threshold_percent = 0.05  # 5% threshold\n",
    "    deviation_threshold = deviations_sorted_desc[ceil(threshold_percent*len(deviations_sorted_desc))]\n",
    "    if sum(deviations > deviation_threshold) == 0:\n",
    "        # in this case, most likely the outliers are not true outliers, but the range of onset is small.\n",
    "        pass\n",
    "    n_nan = group[colname].isna().sum()\n",
    "    group.loc[deviations > deviation_threshold, colname] = np.nan\n",
    "    n_nan_post = group[colname].isna().sum()\n",
    "    #print(f\"{n_nan} -> {n_nan_post} (should be {ceil(threshold_percent*len(deviations_sorted_desc))})\")\n",
    "    return group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e4a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_onset1_outliers = partial(replace_outliers, colname=\"onset1\")\n",
    "replace_onset2_outliers = partial(replace_outliers, colname=\"onset2\")\n",
    "replace_onsetsz_outliers = partial(replace_outliers, colname=\"onset_sz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aacc2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_onsets_df_outliers_removed = all_onsets_df.groupby(\"uuid_extended\").apply(replace_onset1_outliers).groupby(\"uuid_extended\").apply(replace_onset2_outliers).groupby(\"uuid_extended\").apply(replace_onsetsz_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94eab5d",
   "metadata": {},
   "source": [
    "### Overtake removal of the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf39c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_outliers = True\n",
    "if remove_outliers:\n",
    "    all_onsets_df = all_onsets_df_outliers_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78e22e1",
   "metadata": {},
   "source": [
    "## SD speed based on grid approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f029baa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SDSpeedsTileAlgorithm(df_onsets_input, i_wave):  \n",
    "    # i_wave should be 1 or 2\n",
    "    # returns a tuple:\n",
    "    # uuids: a list of the uuids, and a 2d list of velocities: an array of all calculated velocities per session (uuid_extended)\n",
    "    uuids = []\n",
    "    vs_2d = []\n",
    "    \n",
    "    \n",
    "    for i_group, session_group in df_onsets_input[df_onsets_input[f\"onset{i_wave}\"].notna()].groupby(\"uuid_extended\"):\n",
    "        exp_type = session_group[\"exp_type\"].iloc[0]\n",
    "        assert len(session_group[\"exp_type\"].unique()) == 1\n",
    "        tiles_group = session_group.groupby(\"tile\").median()  # TODO: the center values should be mean, not median!\n",
    "        x_y_onset = np.array([tiles_group[\"x\"], tiles_group[\"y\"], tiles_group[\"onset\" + str(i_wave)]])\n",
    "        x_y_onset = x_y_onset.T  # x_y_onset1[i] = [x_i, y_i, onset1_i]\n",
    "        n_tiles = len(x_y_onset)\n",
    "        \n",
    "        # 1. find all tiles with later onset\n",
    "        #      boolean array of arrays: in a row i, value at index j is True if onset j is greater than onset i. \n",
    "        larger_values = x_y_onset[:, 2][:, np.newaxis] < x_y_onset[:, 2]\n",
    "        #      convert True/False into index. Use fact that within a row, i-th element corresponds to index i. Put np.inf if not larger\n",
    "        larger_indices = np.where(larger_values, np.arange(n_tiles), np.inf)\n",
    "        # 2. find all tile distances\n",
    "        dist_matrix = distance_matrix(x_y_onset[:,:2],x_y_onset[:,:2])\n",
    "        #      dist_matrix: each row contains distance to all the other tiles. inf if same tile! (diagonal)\n",
    "        assert (dist_matrix == dist_matrix.T).all()  # symmetric\n",
    "        np.fill_diagonal(dist_matrix, np.inf)  # exclude tile itself from being nearest neighbor\n",
    "        later_tiles_distances = np.where(np.isfinite(larger_indices), dist_matrix, np.inf)\n",
    "        nearest_indices_later_onset = np.argmin(later_tiles_distances, axis=1)\n",
    "        vs = np.zeros(n_tiles)\n",
    "        for i_tile, tile_nearest_indices in enumerate(nearest_indices_later_onset):\n",
    "            if np.isinf(later_tiles_distances[i_tile]).all():  #  a later onset neuron is actually found\n",
    "                continue\n",
    "            else:\n",
    "                i_nearest_later = tile_nearest_indices\n",
    "                ds = dist_matrix[i_tile][i_nearest_later] * 1.579  # objective conversion factor  -> [pixel] * [µm] / [pixel]\n",
    "                dt = (x_y_onset[i_nearest_later][2] - x_y_onset[i_tile][2]) / 15.0  # [frames] / ([frames]/[second]) \n",
    "                vs[i_tile] = ds/dt\n",
    "        vs_2d.append(vs)\n",
    "        uuids.append(i_group)\n",
    "    vs_flat = [item for vs_row in vs_2d for item in vs_row]\n",
    "    v_median = np.median(vs_flat)\n",
    "    print(f\"{v_median} µm/s = {v_median*6./100.} mm/min\") \n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    plt.hist(vs_flat, bins=150)\n",
    "    plt.show()\n",
    "    return (uuids, vs_2d)  # in µm/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ffcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "uuids_grid1, vs_grid1 = SDSpeedsTileAlgorithm(all_onsets_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9dc8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "uuids_grid2, vs_grid2 = SDSpeedsTileAlgorithm(all_onsets_df, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe077415",
   "metadata": {},
   "source": [
    "## SD speed based on cell approach\n",
    "Algorithm stays same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83efc425",
   "metadata": {},
   "outputs": [],
   "source": [
    "later_neurons_distances = [[1., 2.5, np.inf, 2.4], [np.inf, 1.5, np.inf, np.inf], [np.inf, np.inf, np.inf, np.inf], [1.6, 1.8, 2.5, 1.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21391d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_indices_later_onset = np.argsort(later_neurons_distances, axis=1)[:,:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff12324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SDSpeedsCellAlgorithm(df_onsets_input, i_wave, n_neighbors=1, plot_res = False, vectorize = False, onset_sz = False):  \n",
    "    # i_wave should be 1 or 2\n",
    "    # n_neighbors: average the closest n_neighbors cells (with a later onset)\n",
    "    # plot_res: whether to plot the results (histogram with all velocities)\n",
    "    # vectorize: whether to return not only the velocity, but in addition, the 2d vector velocity, as well as the centre of the neuron.\n",
    "    # returns a tuple:\n",
    "    # uuids: a list of the uuids, and a 2d list of velocities: an array of all calculated velocities per session (uuid_extended)\n",
    "    uuids = []\n",
    "    vs_2d = []\n",
    "    neuron_ids = np.array([], dtype=np.int16)\n",
    "    \n",
    "    if onset_sz:\n",
    "        onset_type = \"onset_sz\"\n",
    "    else:\n",
    "        onset_type = \"onset\" + str(i_wave)\n",
    "    \n",
    "    if vectorize:\n",
    "        dx_2d = []\n",
    "        dy_2d = []\n",
    "        centres_x = np.array([])  # the centre coordinate of each neuron. Same as \"x\" column in all_onsets_df.\n",
    "        centres_y = np.array([])\n",
    "    for i_group, session_group in df_onsets_input[df_onsets_input[onset_type].notna()].groupby(\"uuid_extended\"):\n",
    "        # TODO: the center values should be mean, not median!\n",
    "        x_y_onset = np.array([session_group[\"x\"], session_group[\"y\"], session_group[onset_type]])\n",
    "        x_y_onset = x_y_onset.T  # x_y_onset1[i] = [x_i, y_i, onset1_i]\n",
    "        n_neurons = len(x_y_onset)\n",
    "        neuron_ids_curr_session = np.array(session_group[\"neuron_id\"], dtype=np.int16)\n",
    "        neuron_ids = np.concatenate([neuron_ids, neuron_ids_curr_session])\n",
    "        # contains (mean) x/y distance to nearest neighbor for each neuron\n",
    "        dx_session = np.zeros(n_neurons, dtype=np.float64)\n",
    "        dy_session = np.zeros(n_neurons, dtype=np.float64)\n",
    "        \n",
    "        # 1. find all neurons with later onset\n",
    "        #      boolean array of arrays: in a row i, value at index j is True if onset j is greater than onset i. \n",
    "        larger_values = x_y_onset[:, 2][:, np.newaxis] < x_y_onset[:, 2]\n",
    "        #      convert True/False into index. Use fact that within a row, i-th element corresponds to index i. Put np.inf if not larger\n",
    "        larger_indices = np.where(larger_values, np.arange(n_neurons), np.inf)\n",
    "        # 2. find all neuron distances\n",
    "        dist_matrix = distance_matrix(x_y_onset[:,:2],x_y_onset[:,:2])\n",
    "        #      dist_matrix: each row contains distance to all the other tiles. inf if same tile! (diagonal)\n",
    "        assert (dist_matrix == dist_matrix.T).all()  # symmetric\n",
    "        np.fill_diagonal(dist_matrix, np.inf)  # exclude tile itself from being nearest neighbor\n",
    "        # find distances neurons with later onset\n",
    "        later_neurons_distances = np.where(np.isfinite(larger_indices), dist_matrix, np.inf)\n",
    "        # find closest neurons with later onset\n",
    "        nearest_indices_later_onset = np.argsort(later_neurons_distances, axis=1)[:,:n_neighbors]\n",
    "        # calculate velocity with all neighbors above\n",
    "        vs = np.zeros(n_neurons)\n",
    "        if vectorize:\n",
    "            dxs = np.zeros(n_neurons)\n",
    "            dys = np.zeros(n_neurons)\n",
    "        for i_neuron, neuron_nearest_indices in enumerate(nearest_indices_later_onset):\n",
    "            if np.isinf(later_neurons_distances[i_neuron]).all():  #  a later onset neuron is actually found\n",
    "                continue\n",
    "            else:       \n",
    "                if isinstance(neuron_nearest_indices, Iterable):\n",
    "                    v_neighbors_list = np.zeros(len(neuron_nearest_indices))  \n",
    "                    if vectorize:\n",
    "                        dx_neighbors_list = np.zeros(len(neuron_nearest_indices))  \n",
    "                        dy_neighbors_list = np.zeros(len(neuron_nearest_indices))  \n",
    "                    \n",
    "                    for i_neighbor, index_neighbor in enumerate(neuron_nearest_indices):\n",
    "                        ds = dist_matrix[i_neuron][index_neighbor] * 1.579  # objective conversion factor  -> [pixel] * [µm] / [pixel]\n",
    "                        dt = (x_y_onset[index_neighbor][2] - x_y_onset[i_neuron][2]) / 15.0  # [frames] / ([frames]/[second]) \n",
    "                        v_neighbor = ds/dt\n",
    "                        v_neighbors_list[i_neighbor] = v_neighbor\n",
    "                        if vectorize:  # \n",
    "                            # get x, y of current neighbor\n",
    "                            x_nearest = x_y_onset[index_neighbor][0]\n",
    "                            y_nearest = x_y_onset[index_neighbor][1]\n",
    "                            # get x, y of current neuron\n",
    "                            x_curr = x_y_onset[i_neuron][0]\n",
    "                            y_curr = x_y_onset[i_neuron][1]\n",
    "                            # get dx, dy\n",
    "                            dx = x_nearest - x_curr\n",
    "                            dy = y_nearest - y_curr\n",
    "                            dx_neighbors_list[i_neighbor] = dx\n",
    "                            dy_neighbors_list[i_neighbor] = dy\n",
    "                        \n",
    "                        \n",
    "                    vs[i_neuron] = np.median(v_neighbors_list) \n",
    "                    if vectorize:\n",
    "                        dxs[i_neuron] = np.mean(dx_neighbors_list)\n",
    "                        dys[i_neuron] = np.mean(dy_neighbors_list)\n",
    "                        \n",
    "                else:\n",
    "                    ds = dist_matrix[i_neuron][neuron_nearest_indices[0]] * 1.579  # objective conversion factor  -> [pixel] * [µm] / [pixel]\n",
    "                    dt = (x_y_onset[neuron_nearest_indices[0]][2] - x_y_onset[i_neuron][2]) / 15.0  # [frames] / ([frames]/[second]) \n",
    "                    vs[i_neuron] = ds/dt\n",
    "        vs_2d.append(vs)\n",
    "        uuids.append(i_group)\n",
    "        if vectorize:\n",
    "            centres_x = np.concatenate([centres_x, session_group[\"x\"]])\n",
    "            centres_y = np.concatenate([centres_y, session_group[\"y\"]])\n",
    "            dx_2d.append(dxs)\n",
    "            dy_2d.append(dys)\n",
    "            \n",
    "    vs_flat = [item for vs_row in vs_2d for item in vs_row]\n",
    "    v_median = np.median(vs_flat)\n",
    "    print(f\"{v_median} µm/s = {v_median*6./100.} mm/min\") \n",
    "    if plot_res:\n",
    "        fig = plt.figure(figsize=(16,8))\n",
    "        plt.hist(vs_flat, bins=150)\n",
    "        plt.show()\n",
    "    if vectorize:\n",
    "        return (uuids, neuron_ids, vs_2d, dx_2d, dy_2d, centres_x, centres_y)\n",
    "    else:\n",
    "        return (uuids, neuron_ids, vs_2d)  # in µm/s\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07afbbd9",
   "metadata": {},
   "source": [
    "### Set number of neighbors to find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf09e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NEIGHBORS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e85072",
   "metadata": {},
   "outputs": [],
   "source": [
    "uuids_neuron1, ids_neuron1, vs_neuron1 = SDSpeedsCellAlgorithm(all_onsets_df, 1,N_NEIGHBORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9424084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "uuids_neuron2, ids_neuron2, vs_neuron2 = SDSpeedsCellAlgorithm(all_onsets_df, 2,N_NEIGHBORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f49c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "uuids_neuron_sz, ids_neuron_sz, vs_neuron_sz = SDSpeedsCellAlgorithm(all_onsets_df, 2,1,False,False,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2031b729",
   "metadata": {},
   "source": [
    "## Create dataframe from results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6728a7c",
   "metadata": {},
   "source": [
    "### Sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed12c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten all arrays\n",
    "vs_neuron_sz_flat = [element for sublist in vs_neuron_sz for element in sublist]\n",
    "uuids_neuron_sz_flat = [uuids_neuron_sz[i] for i, neurons in enumerate(vs_neuron_sz) for j in range(len(neurons))]\n",
    "assert len(vs_neuron_sz_flat) == len(uuids_neuron_sz_flat)\n",
    "# create mean velocity for all sessions\n",
    "vs_neuron_sz_mean = [np.median(element) for element in vs_neuron_sz]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77209bf3",
   "metadata": {},
   "source": [
    "### SD 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1257ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten all arrays\n",
    "vs_neuron1_flat = [element for sublist in vs_neuron1 for element in sublist]\n",
    "uuids_neuron1_flat = [uuids_neuron1[i] for i, neurons in enumerate(vs_neuron1) for j in range(len(neurons))]\n",
    "assert len(vs_neuron1_flat) == len(uuids_neuron1_flat)\n",
    "# create mean velocity for all sessions\n",
    "vs_neuron1_mean = [np.median(element) for element in vs_neuron1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7317ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid algorithm\n",
    "# flatten all arrays\n",
    "vs_grid1_flat = [element for sublist in vs_grid1 for element in sublist]\n",
    "uuids_grid1_flat = [uuids_grid1[i] for i, tiles in enumerate(vs_grid1) for j in range(len(tiles))]\n",
    "assert len(vs_grid1_flat) == len(uuids_grid1_flat)\n",
    "# create mean velocity for all sessions\n",
    "vs_grid1_mean = [np.median(element) for element in vs_grid1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47347ef0",
   "metadata": {},
   "source": [
    "### SD 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d53eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_neuron2_flat = [element for sublist in vs_neuron2 for element in sublist]\n",
    "uuids_neuron2_flat = [uuids_neuron1[i] for i, neurons in enumerate(vs_neuron2) for j in range(len(neurons))]\n",
    "assert len(vs_neuron2_flat) == len(uuids_neuron2_flat)\n",
    "# create mean velocity for all sessions\n",
    "vs_neuron2_mean = [np.mean(element) for element in vs_neuron2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798bd313",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in vs_neuron2:\n",
    "    print(len(np.isnan(a)))\n",
    "    print(np.mean(a))\n",
    "    print(np.mean(a[~np.isnan(a)]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a5ebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_grid2_flat = [element for sublist in vs_grid2 for element in sublist]\n",
    "uuids_grid2_flat = [uuids_grid1[i] for i, tiles in enumerate(vs_grid2) for j in range(len(tiles))]\n",
    "assert len(vs_grid2_flat) == len(uuids_grid2_flat)\n",
    "# create mean velocity for all sessions\n",
    "vs_grid2_mean = [np.mean(element) for element in vs_grid2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b508eac",
   "metadata": {},
   "source": [
    "## Create data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e1cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid-based algorithm\n",
    "vs_grid_df1 = pd.DataFrame({\"uuid\": uuids_grid1_flat, \"v_umps\": vs_grid1_flat, \"i_wave\": 1})  # all velocities calculated\n",
    "vs_grid_df1_means = pd.DataFrame({\"uuid\": uuids_grid1, \"v_umps\": vs_grid1_mean, \"i_wave\": 1})\n",
    "vs_grid_df2 = pd.DataFrame({\"uuid\": uuids_grid2_flat, \"v_umps\": vs_grid2_flat, \"i_wave\": 2})\n",
    "vs_grid_df2_means = pd.DataFrame({\"uuid\": uuids_grid2, \"v_umps\": vs_grid2_mean, \"i_wave\": 2})\n",
    "\n",
    "# reset index, but keep old index just in case\n",
    "vs_grid_df = pd.concat([vs_grid_df1, vs_grid_df2], axis=0).reset_index()\n",
    "vs_grid_df_means = pd.concat([vs_grid_df1_means, vs_grid_df2_means], axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f10cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuron-based algorithm\n",
    "vs_df1 = pd.DataFrame({\"uuid\": uuids_neuron1_flat, \"v_umps\": vs_neuron1_flat, \"i_wave\": 1})  # all velocities calculated\n",
    "vs_df1_means = pd.DataFrame({\"uuid\": uuids_neuron1, \"v_umps\": vs_neuron1_mean, \"i_wave\": 1})\n",
    "vs_df2 = pd.DataFrame({\"uuid\": uuids_neuron2_flat, \"v_umps\": vs_neuron2_flat, \"i_wave\": 2})  # all velocities calculated\n",
    "vs_df2_means = pd.DataFrame({\"uuid\": uuids_neuron2, \"v_umps\": vs_neuron2_mean, \"i_wave\": 2})\n",
    "\n",
    "vs_df_sz = pd.DataFrame({\"uuid\": uuids_neuron_sz_flat, \"v_umps\": vs_neuron_sz_flat, \"i_wave\": 0})  # all velocities calculated\n",
    "vs_df_sz_means = pd.DataFrame({\"uuid\": uuids_neuron_sz, \"v_umps\": vs_neuron_sz_mean, \"i_wave\": 1})\n",
    "\n",
    "# reset index, but keep old index just in case\n",
    "vs_df = pd.concat([vs_df1, vs_df2], axis=0).reset_index()\n",
    "vs_df_means = pd.concat([vs_df1_means, vs_df2_means], axis=0).reset_index()\n",
    "\n",
    "vs_df_sz = vs_df_sz.reset_index()\n",
    "vs_df_sz_means = vs_df_sz_means.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d26555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extended_to_normal_uuid(uuid):\n",
    "    if \"_\" in uuid:\n",
    "        return uuid.split(\"_\")[0]\n",
    "    else:\n",
    "        return uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c648dc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of 0 values\n",
    "vs_df = vs_df[vs_df[\"v_umps\"] > 0.0]\n",
    "vs_df_sz = vs_df_sz[vs_df_sz[\"v_umps\"] > 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882e5d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: check this warning. Maybe it is due to the split() and it is not a problem?\n",
    "vs_df[\"mouse_id\"] = vs_df.apply(lambda row: ddoc.getMouseIdForUuid(extended_to_normal_uuid(row[\"uuid\"])), axis=1)\n",
    "vs_df_sz[\"mouse_id\"] = vs_df_sz.apply(lambda row: ddoc.getMouseIdForUuid(extended_to_normal_uuid(row[\"uuid\"])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084fe9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_df[\"exp_type\"] = vs_df.apply(lambda row: uuid_exp_type_dict[extended_to_normal_uuid(row[\"uuid\"])], axis=1)\n",
    "vs_grid_df[\"exp_type\"] = vs_grid_df.apply(lambda row: uuid_exp_type_dict[extended_to_normal_uuid(row[\"uuid\"])], axis=1)\n",
    "\n",
    "vs_df_means[\"exp_type\"] = vs_df_means.apply(lambda row: uuid_exp_type_dict[extended_to_normal_uuid(row[\"uuid\"])], axis=1)\n",
    "vs_grid_df_means[\"exp_type\"] = vs_grid_df_means.apply(lambda row: uuid_exp_type_dict[extended_to_normal_uuid(row[\"uuid\"])], axis=1)\n",
    "\n",
    "vs_df_sz[\"exp_type\"] = vs_df_sz.apply(lambda row: uuid_exp_type_dict[extended_to_normal_uuid(row[\"uuid\"])], axis=1)\n",
    "vs_df_sz_means[\"exp_type\"] = vs_df_sz_means.apply(lambda row: uuid_exp_type_dict[extended_to_normal_uuid(row[\"uuid\"])], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db03c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_df_sz[vs_df_sz[\"exp_type\"]==\"tmev\"].mouse_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309757e5",
   "metadata": {},
   "source": [
    "### Add mm/min\n",
    "1 µm/s = 60 µm/min = 0.06 mm/min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abbb034",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERSION_FACTOR = 0.06\n",
    "\n",
    "vs_grid_df[\"v_mmpmin\"] = vs_grid_df[\"v_umps\"] * CONVERSION_FACTOR\n",
    "vs_grid_df_means[\"v_mmpmin\"] = vs_grid_df_means[\"v_umps\"] * CONVERSION_FACTOR\n",
    "vs_df[\"v_mmpmin\"] = vs_df[\"v_umps\"] * CONVERSION_FACTOR\n",
    "vs_df_means[\"v_mmpmin\"] = vs_df_means[\"v_umps\"] * CONVERSION_FACTOR\n",
    "\n",
    "vs_df_sz[\"v_mmpmin\"] = vs_df_sz[\"v_umps\"] * CONVERSION_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9748f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,8))\n",
    "plt.hist(vs_df[(vs_df[\"v_mmpmin\"] < 100) & (vs_df[\"i_wave\"] == 2)][\"v_mmpmin\"], bins=100)\n",
    "plt.xlim((0,50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a12be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset with outliers removed\n",
    "vs_df_sz_outliers_removed = vs_df_sz.copy()\n",
    "for i, g in vs_df_sz_outliers_removed.groupby(\"uuid\"):\n",
    "    count = g.size\n",
    "    drop = int(count*0.05)  # drop lowest and highest 5%\n",
    "    vs_df_sz_outliers_removed.drop(g[\"v_mmpmin\"].nlargest(drop).index, inplace=True)\n",
    "    vs_df_sz_outliers_removed.drop(g[\"v_mmpmin\"].nsmallest(drop).index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a57d419",
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_means_per_session = vs_df_sz_outliers_removed.groupby([\"exp_type\", \"mouse_id\", \"uuid\"]).mean().reset_index() #vs_df_sz.groupby([\"exp_type\", \"mouse_id\", \"uuid\"]).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f067e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: group by uuid and i_wave (pivot_table) and make boxplot with sem\n",
    "means_per_session = vs_df.groupby([\"exp_type\",\"mouse_id\", \"uuid\", \"i_wave\"]).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a8d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,10))\n",
    "plt.suptitle(\"SD speed\")\n",
    "sns.barplot(means_per_session, x=\"exp_type\", y=\"v_mmpmin\", errorbar=\"sd\")\n",
    "plt.tight_layout()\n",
    "if save_figs:\n",
    "    export_fpath_fig = os.path.join(output_folder, f\"mean_sd_speed_by_exp_type_{get_datetime_for_fname()}{file_format}\")\n",
    "    plt.savefig(export_fpath_fig)\n",
    "    print(f\"Saved to {export_fpath_fig}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14972e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,10))\n",
    "plt.suptitle(\"SD speed\")\n",
    "sns.barplot(means_per_session, x=\"i_wave\", y=\"v_mmpmin\", hue=\"exp_type\", errorbar=\"sd\")\n",
    "plt.tight_layout()\n",
    "if save_figs:\n",
    "    export_fpath_fig = os.path.join(output_folder, f\"mean_sd_speed_{get_datetime_for_fname()}{file_format}\")\n",
    "    plt.savefig(export_fpath_fig)\n",
    "    print(f\"Saved to {export_fpath_fig}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8797099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "means_per_session[\"speed_type\"] = \"SD\"\n",
    "sz_means_per_session[\"speed_type\"] = \"Sz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a49ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_speeds = pd.concat([means_per_session[[\"exp_type\", \"mouse_id\", \"uuid\", \"speed_type\", \"i_wave\", \"v_umps\", \"v_mmpmin\"]],\n",
    "sz_means_per_session[[\"exp_type\", \"mouse_id\", \"uuid\", \"speed_type\", \"i_wave\", \"v_umps\", \"v_mmpmin\"]]]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd1916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    export_fpath_df_mean_speeds = os.path.join(output_folder, f\"mean_onset_speed_{get_datetime_for_fname()}.xlsx\")\n",
    "    df_mean_speeds.to_excel(export_fpath_df_mean_speeds, index=False)\n",
    "    print(f\"Saved to {export_fpath_df_mean_speeds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1c930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_df_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca89712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ddoc.getSessionFilesForUuid(\"4ae789df9809469b8668ff01a8cc91ee\")\n",
    "print(d.folder.iloc[0])\n",
    "print(d.nd2.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489bc0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,10))\n",
    "plt.suptitle(\"Sz speed\")\n",
    "sns.barplot(sz_means_per_session, x=\"exp_type\", y=\"v_mmpmin\", hue=\"mouse_id\", errorbar=\"sd\")\n",
    "plt.tight_layout()\n",
    "if save_figs:\n",
    "    export_fpath_fig = os.path.join(output_folder, f\"mean_sz_speed_{get_datetime_for_fname()}{file_format}\")\n",
    "    plt.savefig(export_fpath_fig)\n",
    "    print(f\"Saved to {export_fpath_fig}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53765dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,10))\n",
    "plt.suptitle(\"Sz speed\")\n",
    "sns.barplot(sz_means_per_session, x=\"exp_type\", y=\"v_mmpmin\", errorbar=\"sd\")\n",
    "plt.tight_layout()\n",
    "if save_figs:\n",
    "    export_fpath_fig = os.path.join(output_folder, f\"mean_sz_speed_by_exp_type_{get_datetime_for_fname()}{file_format}\")\n",
    "    plt.savefig(export_fpath_fig)\n",
    "    print(f\"Saved to {export_fpath_fig}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,8))\n",
    "plt.hist(vs_df[(vs_df[\"v_mmpmin\"] < 100)][\"v_mmpmin\"], bins=100)\n",
    "plt.xlim((0,50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91efd017",
   "metadata": {},
   "outputs": [],
   "source": [
    "means_per_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653fe420",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e20309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
