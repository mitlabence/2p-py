{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0af67c1",
   "metadata": {},
   "source": [
    "# Important\n",
    "In raw_traces, cumulative data, like total distance, time, ... are not matched between segments! I.e. expect jumps in the data. This should be circumvented in the assembled traces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdba252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auto-reload modules (used to develop functions outside this notebook)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b9ef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import labrotation.file_handling as fh\n",
    "import datadoc_util as ddutil\n",
    "from labrotation import two_photon_session as tps\n",
    "import h5py\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ee2435",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_BL_AM_FRAMES = 5000  # take 5000 frames before and after event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2cd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_dict = dict()\n",
    "if not os.path.exists(\"./.env\"):\n",
    "    print(\".env does not exist\")\n",
    "else:\n",
    "    with open(\"./.env\", \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            l = line.rstrip().split(\"=\")\n",
    "            env_dict[l[0]] = l[1]\n",
    "print(env_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db02f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddoc = ddutil.DataDocumentation(env_dict[\"DATA_DOCU_FOLDER\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef243e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddoc.loadDataDoc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d7b78c",
   "metadata": {},
   "source": [
    "### Define function for saving file with date time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb36f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datetime_for_fname():\n",
    "    now = dt.now()\n",
    "    return f\"{now.year:04d}{now.month:02d}{now.day:02d}-{now.hour:02d}{now.minute:02d}{now.second:02d}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2748ac",
   "metadata": {},
   "source": [
    "### Open excel file as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc9d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "matlab_2p_folder = env_dict[\"MATLAB_2P_FOLDER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a51b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_list_fpath = os.path.join(env_dict[\"DATA_DOCU_FOLDER\"], \"events_list.xlsx\")\n",
    "assert os.path.exists(events_list_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafacbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events = pd.read_excel(events_list_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec33c931",
   "metadata": {},
   "source": [
    "### Filter dataframe\n",
    "Only take seizure events (as of July 2023, only seizures/sz, and seizures with not enough baseline/sz_invalid_bl exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0ba86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events = df_events[df_events[\"event_type\"] == \"sz\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb8b7df",
   "metadata": {},
   "source": [
    "### Add length of interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4102da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events[\"interval_length\"] = df_events[\"end_frame\"] - df_events[\"begin_frame\"] + 1  # begin and end frames both inclusive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b5c0b6",
   "metadata": {},
   "source": [
    "# Get corresponding traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d008b4",
   "metadata": {},
   "source": [
    "## Open directory where the files are located\n",
    "Only necessary when there is no access to the server (the locations set in Data Documentation grouping files). The files should be grouped by folder, each folder named as the session uuid. If there was no lfp, add \\_nolfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d820e0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_folder = fh.open_dir(\"Open folder containing uuid folders, each containing session files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9333b825",
   "metadata": {},
   "source": [
    "### Check that all necessary folders exist\n",
    "If it does not throw an error, it means everything should be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68117f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for uuid in df_events[\"recording_uuid\"].unique():\n",
    "    \n",
    "    if not (os.path.exists(os.path.join(traces_folder, uuid)) or os.path.exists(os.path.join(traces_folder, f\"{uuid}_nolfp\"))):\n",
    "        print(uuid)\n",
    "        mouse_id = ddoc.getMouseIdForUuid(uuid)\n",
    "        nik_name = ddoc.getNikonFileNameForUuid(uuid)\n",
    "        print(f\"\\t{mouse_id} {nik_name}\")\n",
    "    else:\n",
    "        session_folder = os.path.join(traces_folder, uuid)\n",
    "        has_lfp = True\n",
    "        if not os.path.exists(session_folder):\n",
    "            session_folder = os.path.join(traces_folder, f\"{uuid}_nolfp\")\n",
    "            has_lfp = False\n",
    "        \n",
    "        files = ddoc.getSessionFilesForUuuid(uuid).iloc[0].to_dict()\n",
    "        \n",
    "        fname_nd2 = files[\"nd2\"]\n",
    "        assert os.path.exists(os.path.join(session_folder, fname_nd2))\n",
    "        \n",
    "        fname_nikmeta = files[\"nikon_meta\"]\n",
    "        assert os.path.exists(os.path.join(session_folder, fname_nikmeta))\n",
    "         \n",
    "        fname_lv = files[\"labview\"]\n",
    "        assert os.path.exists(os.path.join(session_folder, fname_lv))\n",
    "        \n",
    "        fname_lvtime = os.path.splitext(files[\"labview\"])[0] + \"time.txt\"\n",
    "        assert os.path.exists(os.path.join(session_folder, fname_lvtime))\n",
    "        \n",
    "        fname_lfp = None\n",
    "        if has_lfp:\n",
    "            fname_lfp = files[\"lfp\"]\n",
    "            assert os.path.exists(os.path.join(session_folder, fname_lfp))\n",
    "print(\"Consistency check done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818da7e1",
   "metadata": {},
   "source": [
    "## Assemble traces from multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec94a8f",
   "metadata": {},
   "source": [
    "### Get manual LFP shift values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea792f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_corrections_fpath = os.path.join(env_dict[\"DATA_DOCU_FOLDER\"], \"lfp_corrections.xlsx\")\n",
    "assert os.path.exists(lfp_corrections_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86ea827",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lfp_corrections = pd.read_excel(lfp_corrections_fpath, index_col=\"uuid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953441cc",
   "metadata": {},
   "source": [
    "### Open all sessions that appear in the dataset. Do the matching of traces\n",
    "WARNING: this takes long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d8049",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_dict = dict()\n",
    "has_lfp_dict = dict()\n",
    "for uuid in df_events[\"recording_uuid\"].unique():\n",
    "    print(uuid)\n",
    "    session_folder = os.path.join(traces_folder, uuid)\n",
    "    has_lfp = True\n",
    "    if not os.path.exists(session_folder):\n",
    "        session_folder = os.path.join(traces_folder, f\"{uuid}_nolfp\")\n",
    "        has_lfp = False\n",
    "    files = ddoc.getSessionFilesForUuuid(uuid).iloc[0].to_dict()\n",
    "    \n",
    "    fname_nd2 = files[\"nd2\"]\n",
    "    fpath_nd2 = os.path.join(session_folder, fname_nd2)\n",
    "    \n",
    "    fname_nikmeta = files[\"nikon_meta\"]\n",
    "    fpath_nikmeta = os.path.join(session_folder, fname_nikmeta)\n",
    "    \n",
    "    fname_lv = files[\"labview\"]\n",
    "    fname_lvtime = os.path.splitext(files[\"labview\"])[0] + \"time.txt\"\n",
    "    fpath_lv = os.path.join(session_folder, fname_lv)\n",
    "    fpath_lvtime = os.path.join(session_folder, fname_lvtime)\n",
    "    \n",
    "    fname_lfp = None\n",
    "    fpath_lfp = None\n",
    "    if has_lfp:\n",
    "        fname_lfp = files[\"lfp\"]\n",
    "        fpath_lfp = os.path.join(session_folder, fname_lfp)\n",
    "    \n",
    "    session = tps.TwoPhotonSession.init_and_process(fpath_nd2, fpath_nikmeta, fpath_lv, fpath_lvtime, fpath_lfp, matlab_2p_folder)\n",
    "    if session.has_lfp():\n",
    "        t_lfp_shift = df_lfp_corrections.loc[uuid].lfp_manual_delay\n",
    "        session.shift_lfp(t_lfp_shift)\n",
    "        print(f\"\\t\\t{uuid} LFP shifted by {t_lfp_shift} s\")\n",
    "    else:\n",
    "        print(f\"\\t\\t{uuid} has no LFP\")\n",
    "    sessions_dict[uuid] = session\n",
    "    has_lfp_dict[uuid] = session.has_lfp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd46f6e7",
   "metadata": {},
   "source": [
    "### Group by event. Loop over entries, extract corresponding frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4899e515",
   "metadata": {},
   "source": [
    "### Test that baseline and aftermath add up to 5000 frames each in all cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd30a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (event_uuid, ival_type), g in df_events.groupby([\"event_uuid\", \"interval_type\"]):\n",
    "    if ival_type in [\"bl\", \"am\"]:\n",
    "        assert g[\"interval_length\"].sum() == LEN_BL_AM_FRAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2fb03b",
   "metadata": {},
   "source": [
    "### Extract traces\n",
    "Define data structure (dict):\n",
    "\n",
    "* event_uuid as outermost key\n",
    "* interval_type (bl, sz, am) as next key\n",
    "* event_index as next key\n",
    "* inside event_index, the various traces are stored as key: array pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e12b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_traces_dict = dict()\n",
    "sessions_nik_tstamps_dict = dict()\n",
    "for event_uuid, g_uuid in df_events.groupby(\"event_uuid\"):\n",
    "    \n",
    "    sessions_traces_dict[event_uuid] = dict()\n",
    "    sessions_nik_tstamps_dict[event_uuid] = dict()\n",
    "    \n",
    "    for ival_type, g in g_uuid.groupby(\"interval_type\"):\n",
    "        sessions_traces_dict[event_uuid][ival_type] = dict()\n",
    "        sessions_nik_tstamps_dict[event_uuid][ival_type] = dict()\n",
    "        \n",
    "        # loop through all segments (parts of different recordings) making up an interval (baseline, sz, aftermath)\n",
    "        for i_segment, segment in g.sort_values(by=[\"event_index\"]).iterrows():\n",
    "            # find correct TwoPhotonSession object\n",
    "            segment_uuid = segment[\"recording_uuid\"]\n",
    "            segment_session = sessions_dict[segment_uuid]\n",
    "            \n",
    "            # get labview data\n",
    "            lv_dist = segment_session.belt_scn_dict['distance']\n",
    "            lv_speed = segment_session.belt_scn_dict['speed']\n",
    "            lv_running = segment_session.belt_scn_dict['running']\n",
    "            lv_totdist = segment_session.belt_scn_dict['totdist']\n",
    "            lv_rounds = segment_session.belt_scn_dict['rounds']\n",
    "            lv_t = segment_session.belt_scn_dict['tsscn']/1000.  # switch from ms to s\n",
    "            \n",
    "            # get nikon data\n",
    "            mean_fluo = segment_session.mean_fluo\n",
    "            \n",
    "            # get lfp data\n",
    "            # lfp already matched to labview. lfp and movement channels t values should be same, but save them to be sure\n",
    "            \n",
    "            if segment_session.has_lfp():\n",
    "                lfp_t, lfp_y = segment_session.lfp_lfp()\n",
    "                lfp_mov_t, lfp_mov_y = segment_session.lfp_movement()\n",
    "            else:\n",
    "                lfp_t = lv_t.copy()\n",
    "                lfp_mov_t = lv_t.copy()\n",
    "                \n",
    "                lfp_y = np.zeros(len(lfp_t))\n",
    "                lfp_mov_y = np.zeros(len(lfp_t))\n",
    "            \n",
    "            # cut to segment size\n",
    "            begin_frame = segment[\"begin_frame\"] - 1  # switch to 0-indexing\n",
    "            end_frame = segment[\"end_frame\"]  # array[begin_frame:end_frame] does not include end_frame, so keep it shifted to right by 1\n",
    "            \n",
    "            \n",
    "            # match lfp time to frames first\n",
    "            t_begin = lv_t[begin_frame]\n",
    "            \n",
    "            if end_frame < len(mean_fluo):  # avoid skipping a lot of data points between adjacent frames\n",
    "                t_end = lv_t[end_frame]\n",
    "            else:\n",
    "                t_end = lv_t[end_frame-1]\n",
    "            \n",
    "            lfp_t_flags = np.where((lfp_t > t_begin) & (lfp_t < t_end))[0]\n",
    "            lfp_mov_t_flags = np.where((lfp_mov_t > t_begin) & (lfp_mov_t < t_end))[0]\n",
    "            \n",
    "            lfp_t = lfp_t[lfp_t_flags]\n",
    "            lfp_y = lfp_y[lfp_t_flags]\n",
    "            \n",
    "            lfp_mov_t = lfp_mov_t[lfp_mov_t_flags]\n",
    "            lfp_mov_y = lfp_mov_y[lfp_mov_t_flags]\n",
    "            \n",
    "            # cut all data that is matched to nikon frames\n",
    "            lv_dist = lv_dist[begin_frame:end_frame]\n",
    "            lv_speed = lv_speed[begin_frame:end_frame]\n",
    "            lv_running = np.array(lv_running[begin_frame:end_frame], dtype=np.uint8)\n",
    "            lv_totdist = lv_totdist[begin_frame:end_frame]\n",
    "            lv_rounds = np.array(lv_rounds[begin_frame:end_frame], dtype=np.uint8)\n",
    "            lv_t = lv_t[begin_frame:end_frame]\n",
    "            \n",
    "            mean_fluo = mean_fluo[begin_frame:end_frame]\n",
    "            \n",
    "            # get Nikon time stamps of first and last frame in segment\n",
    "            nik_tstamp_begin = segment_session.nikon_time_stamp(begin_frame)  # begin frame is in zero indexing, so no change\n",
    "            nik_tstamp_end = segment_session.nikon_time_stamp(end_frame-1)  # end frame marks open interval, need to point to true last element\n",
    "\n",
    "            \n",
    "            # create dictionary of the data\n",
    "            data_dict = dict()  # create a dict with the name-array pairs for each data source\n",
    "            \n",
    "            data_dict[\"lv_dist\"] = lv_dist\n",
    "            data_dict[\"lv_speed\"] = lv_speed\n",
    "            data_dict[\"lv_running\"] = lv_running\n",
    "            data_dict[\"lv_totdist\"] = lv_totdist\n",
    "            data_dict[\"lv_rounds\"] = lv_rounds\n",
    "            data_dict[\"lv_t_s\"] = lv_t  # mark units to emphasize switching from ms to s\n",
    "            \n",
    "            data_dict[\"mean_fluo\"] = mean_fluo\n",
    "            \n",
    "            data_dict[\"lfp_t\"] = lfp_t\n",
    "            data_dict[\"lfp_y\"] = lfp_y\n",
    "            data_dict[\"lfp_mov_t\"] = lfp_mov_t\n",
    "            data_dict[\"lfp_mov_y\"] = lfp_mov_y\n",
    "            \n",
    "            # copy dictionary into large dict\n",
    "            sessions_traces_dict[event_uuid][ival_type][segment[\"event_index\"]] = data_dict.copy()\n",
    "            sessions_nik_tstamps_dict[event_uuid][ival_type][segment[\"event_index\"]] = (nik_tstamp_begin, nik_tstamp_end)           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3889e13",
   "metadata": {},
   "source": [
    "# Save raw traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b6e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = fh.open_dir(\"Choose export directory for results!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d38fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fpath = os.path.join(output_dir, f\"raw_traces_{get_datetime_for_fname()}.h5\")\n",
    "print(f\"Saving to {output_fpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c92f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(output_fpath, \"w\") as hf:\n",
    "    for event_uuid in sessions_traces_dict.keys():\n",
    "        uuid_grp = hf.create_group(event_uuid)\n",
    "        for interval_type in sessions_traces_dict[event_uuid].keys():\n",
    "            ival_type_grp = uuid_grp.create_group(interval_type)\n",
    "            for event_index in sessions_traces_dict[event_uuid][interval_type].keys():\n",
    "                event_ind_grp = ival_type_grp.create_group(str(event_index))\n",
    "                for trace_name in sessions_traces_dict[event_uuid][interval_type][event_index].keys():\n",
    "                    event_ind_grp.create_dataset(trace_name, data=sessions_traces_dict[event_uuid][interval_type][event_index][trace_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7288c7",
   "metadata": {},
   "source": [
    "# Create joint traces\n",
    "Create dictionary with event_uuid as outermost key. Inside it, there should be the different named arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a43294",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.array([1, 2, 2.5, 5])\n",
    "arr2 = np.array([0.1, 0.2, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858cf767",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1*arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2befe753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dt(t):\n",
    "    t1 = t[1:]\n",
    "    t0 = t[:-1]\n",
    "    dt = np.zeros(len(t))\n",
    "    dt[1:] = t1 - t0\n",
    "    dt[0] = dt[1]  # assume same step size to avoid 0\n",
    "    return dt\n",
    "def create_totdist_abs(speed, dt):\n",
    "    totdist_abs = np.zeros(len(speed))\n",
    "    totdist_abs[0] = speed[0]*dt[0]\n",
    "    for i in range(1, len(totdist_abs)):\n",
    "        totdist_abs[i] = totdist_abs[i-1] + abs(speed[i]*dt[i])\n",
    "    return totdist_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082fcab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_session_traces_dict = dict()\n",
    "joint_session_metadata_dict = dict()\n",
    "\n",
    "t_last_segment_nik_end = None # for creating gaps where seizure recordings were interrupted\n",
    "previous_recording_uuid = None #\n",
    "\n",
    "for event_uuid in sessions_traces_dict.keys():\n",
    "    print(event_uuid)\n",
    "    lv_dist = np.array([])\n",
    "    lv_speed = np.array([])\n",
    "    lv_running = np.array([], dtype=np.uint8)\n",
    "    lv_totdist = np.array([])\n",
    "    lv_rounds = np.array([], dtype=np.uint8)\n",
    "    lv_totdist_abs = np.array([])\n",
    "    lv_dt = np.array([])\n",
    "    lv_t_s = np.array([])\n",
    "    mean_fluo = np.array([])\n",
    "    lfp_t = np.array([])\n",
    "    lfp_y = np.array([])\n",
    "    lfp_mov_t = np.array([])\n",
    "    lfp_mov_y = np.array([])\n",
    "    \n",
    "    # break_points: the first frame index of each patch (i.e. piece of video that makes up baseline, ...)\n",
    "    # break_points_lfp: the corresponding first frame index of each segment in lfp\n",
    "    # segment_type_break_points: the first frame index of each segment (bl, sz, am)\n",
    "    # segment_type_break_points_lfp: same for lfp\n",
    "    event_metadata_dict = {\"break_points\": [], \"break_points_lfp\": [], \"segment_type_break_points\":[], \"segment_type_break_points_lfp\":[], \"recording_break_points\":[], \"recording_break_points_lfp\":[]}\n",
    "    \n",
    "    # frame 0 is the first index of the first segment/patch\n",
    "    event_metadata_dict[\"break_points\"].append(0)\n",
    "    event_metadata_dict[\"break_points_lfp\"].append(0)\n",
    "    event_metadata_dict[\"segment_type_break_points\"].append(0)\n",
    "    event_metadata_dict[\"segment_type_break_points_lfp\"].append(0)\n",
    "    event_metadata_dict[\"recording_break_points\"].append(0)\n",
    "    event_metadata_dict[\"recording_break_points_lfp\"].append(0)\n",
    "    \n",
    "    i_segment = 0  # keep count for checking consistency\n",
    "    for bl_event_index in sorted(sessions_traces_dict[event_uuid][\"bl\"].keys()):\n",
    "        #print(f\"{i_segment} {bl_event_index}\")\n",
    "        assert bl_event_index == i_segment\n",
    "        # concatenate data that need not be matched to previous segment\n",
    "        lv_speed_curr = np.array(sessions_traces_dict[event_uuid][\"bl\"][bl_event_index][\"lv_speed\"])  # also need for totdist_abs\n",
    "        lv_dist = np.concatenate([lv_dist, np.array(sessions_traces_dict[event_uuid][\"bl\"][bl_event_index][\"lv_dist\"])])\n",
    "        lv_speed = np.concatenate([lv_speed, lv_speed_curr])\n",
    "        lv_running = np.concatenate([lv_running, np.array(sessions_traces_dict[event_uuid][\"bl\"][bl_event_index][\"lv_running\"])])\n",
    "        \n",
    "        \n",
    "        mean_fluo = np.concatenate([mean_fluo, np.array(sessions_traces_dict[event_uuid][\"bl\"][bl_event_index][\"mean_fluo\"])])\n",
    "        lfp_y = np.concatenate([lfp_y, np.array(sessions_traces_dict[event_uuid][\"bl\"][bl_event_index][\"lfp_y\"])])\n",
    "        lfp_mov_y = np.concatenate([lfp_mov_y, np.array(sessions_traces_dict[event_uuid][\"bl\"][bl_event_index][\"lfp_mov_y\"])])\n",
    "        \n",
    "        # handle time values differently: need to adjust them such that all start from 0, and \n",
    "        # beginning of segment is matched to end of previous segment\n",
    "        lv_t_s_curr = np.array(sessions_traces_dict[event_uuid][\"bl\"][bl_event_index][\"lv_t_s\"])\n",
    "        lfp_t_curr = np.array(sessions_traces_dict[event_uuid][\"bl\"][bl_event_index][\"lfp_t\"])\n",
    "        lfp_mov_t_curr = np.array(sessions_traces_dict[event_uuid][\"bl\"][bl_event_index][\"lfp_mov_t\"])\n",
    "        \n",
    "        current_recording_uuid = df_events[(df_events[\"event_uuid\"] == event_uuid) & (df_events[\"event_index\"] == bl_event_index)].recording_uuid.values[0]\n",
    "        # handle cumulative quantities differently: match them to last element\n",
    "        lv_totdist_curr = np.array(sessions_traces_dict[event_uuid][\"bl\"][bl_event_index][\"lv_totdist\"])\n",
    "        lv_rounds_curr = np.array(sessions_traces_dict[event_uuid][\"bl\"][bl_event_index][\"lv_rounds\"])\n",
    "        lv_dt_curr = np.array(create_dt(sessions_traces_dict[event_uuid][\"bl\"][bl_event_index][\"lv_t_s\"]))\n",
    "        lv_totdist_abs_curr = np.array(create_totdist_abs(lv_speed_curr, lv_dt_curr))\n",
    "        \n",
    "        \n",
    "        # dt is not cumulative\n",
    "        lv_dt = np.concatenate([lv_dt, lv_dt_curr])\n",
    "        \n",
    "        if len(lv_t_s) == 0:  # first entry, shift to 0 start\n",
    "            lv_t_s = np.concatenate([lv_t_s, lv_t_s_curr - lv_t_s_curr[0]])\n",
    "            lfp_t = np.concatenate([lfp_t, lfp_t_curr - lfp_t_curr[0]])\n",
    "            lfp_mov_t = np.concatenate([lfp_mov_t, lfp_mov_t_curr - lfp_mov_t_curr[0]])\n",
    "            # no need to match cumulative data\n",
    "            lv_totdist = np.concatenate([lv_totdist, lv_totdist_curr])\n",
    "            lv_rounds = np.concatenate([lv_rounds, lv_rounds_curr])\n",
    "            \n",
    "            lv_totdist_abs = np.concatenate([lv_totdist_abs, lv_totdist_abs_curr])\n",
    "            \n",
    "        else:  # match current segment to end of previous segment\n",
    "            # add break point info first\n",
    "            if (current_recording_uuid != previous_recording_uuid) and (previous_recording_uuid is not None):\n",
    "                event_metadata_dict[\"recording_break_points\"].append(len(lv_t_s))\n",
    "                event_metadata_dict[\"recording_break_points_lfp\"].append(len(lfp_t))\n",
    "            event_metadata_dict[\"break_points\"].append(len(lv_t_s))\n",
    "            event_metadata_dict[\"break_points_lfp\"].append(len(lfp_t))\n",
    "            # get average time step in the lfp/lv data, take this as the time step between previous segment last and current\n",
    "            # segment first entry\n",
    "            dt_lv_t_s_avg = (lv_t_s_curr[1:] - lv_t_s_curr[:-1]).mean()\n",
    "            dt_lfp_t_avg = (lfp_t_curr[1:] - lfp_t_curr[:-1]).mean()\n",
    "            dt_lfp_mov_t_avg = (lfp_mov_t_curr[1:] - lfp_mov_t_curr[:-1]).mean()\n",
    "            # once we have dt, need to modify arrays by (-[0] + dt)\n",
    "            lv_t_s = np.concatenate([lv_t_s, lv_t_s_curr - lv_t_s_curr[0] + lv_t_s[-1] + dt_lv_t_s_avg])\n",
    "            lfp_t = np.concatenate([lfp_t, lfp_t_curr - lfp_t_curr[0] + lfp_t[-1] + dt_lfp_t_avg])\n",
    "            lfp_mov_t = np.concatenate([lfp_mov_t, lfp_mov_t_curr - lfp_mov_t_curr[0] + lfp_mov_t[-1] + dt_lfp_mov_t_avg])\n",
    "            lv_dt = np.concatenate([lv_dt, lv_dt_curr])\n",
    "            # match cumulative data to last element\n",
    "            lv_totdist_last = lv_totdist[-1]\n",
    "            lv_rounds_last = lv_rounds[-1]\n",
    "            lv_totdist_abs_last = lv_totdist_abs[-1]\n",
    "            lv_totdist = np.concatenate([lv_totdist, lv_totdist_curr - lv_totdist_curr[0] + lv_totdist_last])\n",
    "            lv_rounds = np.concatenate([lv_rounds, lv_rounds_curr - lv_rounds_curr[0] + lv_rounds_last])\n",
    "            lv_totdist_abs = np.concatenate([lv_totdist_abs, lv_totdist_abs_curr - lv_totdist_abs_curr[0] + lv_totdist_abs_last])\n",
    "            \n",
    "        previous_recording_uuid = current_recording_uuid\n",
    "        \n",
    "        # currently no need for bl timestamps, but implement the algorithm just in case in future\n",
    "        # the gaps should be also included in baseline\n",
    "        # get end timestamp from the tuple (nik_tstamp_begin, nik_tstamp_end) \n",
    "        t_last_segment_nik_end = sessions_nik_tstamps_dict[event_uuid][\"bl\"][bl_event_index][1] \n",
    "               \n",
    "        i_segment += 1\n",
    "        \n",
    "    # add first frame index of sz segment\n",
    "    event_metadata_dict[\"segment_type_break_points\"].append(len(lv_t_s))\n",
    "    event_metadata_dict[\"segment_type_break_points_lfp\"].append(len(lfp_t))\n",
    "    \n",
    "    \n",
    "    # only include gap for seizures where the seizure started during the break between the two recordings\n",
    "    if event_uuid == \"f0442bebcd1a4291a8d0559eb47df08e\":\n",
    "        t_last_segment_nik_end += timedelta(seconds=60)\n",
    "    elif event_uuid == \"f481149fa8694621be6116cb84ae2d3c\":\n",
    "        t_last_segment_nik_end += timedelta(seconds=28)\n",
    "    elif event_uuid == \"7753b03a2a554cccaab42f1c0458d742\":\n",
    "        t_last_segment_nik_end += timedelta(seconds=150)\n",
    "    elif event_uuid == \"54c31c3151944cfd86043932d3a19b9a\":\n",
    "        t_last_segment_nik_end += timedelta(seconds=115)\n",
    "    else:\n",
    "        t_last_segment_nik_end = None\n",
    "            \n",
    "    \n",
    "    for sz_event_index in sorted(sessions_traces_dict[event_uuid][\"sz\"].keys()):\n",
    "        #print(f\"{i_segment} {sz_event_index}\")\n",
    "        assert sz_event_index == i_segment\n",
    "        lv_speed_curr = np.array(sessions_traces_dict[event_uuid][\"sz\"][sz_event_index][\"lv_speed\"])\n",
    "        \n",
    "        lv_dist = np.concatenate([lv_dist, np.array(sessions_traces_dict[event_uuid][\"sz\"][sz_event_index][\"lv_dist\"])])\n",
    "        lv_speed = np.concatenate([lv_speed, lv_speed_curr])\n",
    "        lv_running = np.concatenate([lv_running, np.array(sessions_traces_dict[event_uuid][\"sz\"][sz_event_index][\"lv_running\"])])\n",
    "        #lv_t_s = np.concatenate([lv_t_s, sessions_traces_dict[event_uuid][\"sz\"][sz_event_index][\"lv_t_s\"]])\n",
    "        mean_fluo = np.concatenate([mean_fluo, np.array(sessions_traces_dict[event_uuid][\"sz\"][sz_event_index][\"mean_fluo\"])])\n",
    "        #lfp_t = np.concatenate([lfp_t, sessions_traces_dict[event_uuid][\"sz\"][sz_event_index][\"lfp_t\"]])\n",
    "        lfp_y = np.concatenate([lfp_y, np.array(sessions_traces_dict[event_uuid][\"sz\"][sz_event_index][\"lfp_y\"])])\n",
    "        #lfp_mov_t = np.concatenate([lfp_mov_t, sessions_traces_dict[event_uuid][\"sz\"][sz_event_index][\"lfp_mov_t\"]])\n",
    "        lfp_mov_y = np.concatenate([lfp_mov_y, np.array(sessions_traces_dict[event_uuid][\"sz\"][sz_event_index][\"lfp_mov_y\"])])\n",
    "\n",
    "        # match cumulative data to last element\n",
    "        lv_totdist_curr = np.array(sessions_traces_dict[event_uuid][\"sz\"][sz_event_index][\"lv_totdist\"])\n",
    "        lv_rounds_curr = np.array(sessions_traces_dict[event_uuid][\"sz\"][sz_event_index][\"lv_rounds\"])\n",
    "        lv_dt_curr = np.array(create_dt(sessions_traces_dict[event_uuid][\"sz\"][sz_event_index][\"lv_t_s\"]))\n",
    "        lv_totdist_abs_curr = np.array(create_totdist_abs(lv_speed_curr, lv_dt_curr))\n",
    "        \n",
    "        \n",
    "        lv_totdist_last = lv_totdist[-1]\n",
    "        lv_rounds_last = lv_rounds[-1]\n",
    "        lv_totdist_abs_last = lv_totdist_abs[-1]\n",
    "        \n",
    "        lv_totdist = np.concatenate([lv_totdist, lv_totdist_curr - lv_totdist_curr[0] + lv_totdist_last])\n",
    "        lv_rounds = np.concatenate([lv_rounds, lv_rounds_curr - lv_rounds_curr[0] + lv_rounds_last])\n",
    "        lv_totdist_abs = np.concatenate([lv_totdist_abs, lv_totdist_abs_curr - lv_totdist_abs_curr[0] + lv_totdist_abs_last])\n",
    "        \n",
    "        lv_t_s_curr = np.array(sessions_traces_dict[event_uuid][\"sz\"][sz_event_index][\"lv_t_s\"])\n",
    "        lfp_t_curr = np.array(sessions_traces_dict[event_uuid][\"sz\"][sz_event_index][\"lfp_t\"])\n",
    "        lfp_mov_t_curr = np.array(sessions_traces_dict[event_uuid][\"sz\"][sz_event_index][\"lfp_mov_t\"])\n",
    "        # use same method of matching to end of previous segment as in bl\n",
    "        dt_lv_t_s_avg = (lv_t_s_curr[1:] - lv_t_s_curr[:-1]).mean()\n",
    "        dt_lfp_t_avg = (lfp_t_curr[1:] - lfp_t_curr[:-1]).mean()\n",
    "        dt_lfp_mov_t_avg = (lfp_mov_t_curr[1:] - lfp_mov_t_curr[:-1]).mean()\n",
    "        \n",
    "        # add break points\n",
    "        current_recording_uuid = df_events[(df_events[\"event_uuid\"] == event_uuid) & (df_events[\"event_index\"] == sz_event_index)].recording_uuid.values[0]\n",
    "        if (current_recording_uuid != previous_recording_uuid) and (previous_recording_uuid is not None):\n",
    "            event_metadata_dict[\"recording_break_points\"].append(len(lv_t_s))\n",
    "            event_metadata_dict[\"recording_break_points_lfp\"].append(len(lfp_t))\n",
    "        previous_recording_uuid = current_recording_uuid\n",
    "        event_metadata_dict[\"break_points\"].append(len(lv_t_s))\n",
    "        event_metadata_dict[\"break_points_lfp\"].append(len(lfp_t))\n",
    "        \n",
    "        if t_last_segment_nik_end is None:\n",
    "            # once we have dt, need to modify arrays by (-current[0] + last[-1] + dt)\n",
    "            lv_t_s = np.concatenate([lv_t_s, lv_t_s_curr - lv_t_s_curr[0] + lv_t_s[-1] + dt_lv_t_s_avg])\n",
    "            lfp_t = np.concatenate([lfp_t, lfp_t_curr - lfp_t_curr[0] + lfp_t[-1] + dt_lfp_t_avg])\n",
    "            lfp_mov_t = np.concatenate([lfp_mov_t, lfp_mov_t_curr - lfp_mov_t_curr[0] + lfp_mov_t[-1] + dt_lfp_mov_t_avg])\n",
    "        else:\n",
    "            # for sz only (for now), add to dt the time difference between the last frame of the previous segment and\n",
    "            # the first frame of the current segment\n",
    "            t_current_segment_nik_start = sessions_nik_tstamps_dict[event_uuid][\"sz\"][sz_event_index][0] \n",
    "            dt_nik_frames = abs(t_current_segment_nik_start - t_last_segment_nik_end).total_seconds()\n",
    "            \n",
    "            # introduce possible gap coming from session split up in two nikon recordings:\n",
    "            # lv_t_s is from belt_scn_dict, i.e. time poitns are in scanner time frame\n",
    "            lv_t_s = np.concatenate([lv_t_s, lv_t_s_curr - lv_t_s_curr[0] + lv_t_s[-1] + dt_nik_frames])\n",
    "            lfp_t = np.concatenate([lfp_t, lfp_t_curr - lfp_t_curr[0] + lfp_t[-1] + dt_nik_frames])\n",
    "            lfp_mov_t = np.concatenate([lfp_mov_t, lfp_mov_t_curr - lfp_mov_t_curr[0] + lfp_mov_t[-1] + dt_nik_frames])\n",
    "        \n",
    "        t_last_segment_nik_end = sessions_nik_tstamps_dict[event_uuid][\"sz\"][sz_event_index][1] \n",
    "        \n",
    "        i_segment += 1\n",
    "    # add first frame index of am segment\n",
    "    event_metadata_dict[\"segment_type_break_points\"].append(len(lv_t_s))\n",
    "    event_metadata_dict[\"segment_type_break_points_lfp\"].append(len(lfp_t))\n",
    "    \n",
    "    \n",
    "    for am_event_index in sorted(sessions_traces_dict[event_uuid][\"am\"].keys()):\n",
    "        #print(f\"{i_segment} {am_event_index}\")\n",
    "        assert am_event_index == i_segment\n",
    "        lv_speed_curr = np.array(sessions_traces_dict[event_uuid][\"am\"][am_event_index][\"lv_speed\"])\n",
    "        \n",
    "        lv_dist = np.concatenate([lv_dist, np.array(sessions_traces_dict[event_uuid][\"am\"][am_event_index][\"lv_dist\"])])\n",
    "        lv_speed = np.concatenate([lv_speed, lv_speed_curr])\n",
    "        lv_running = np.concatenate([lv_running, np.array(sessions_traces_dict[event_uuid][\"am\"][am_event_index][\"lv_running\"])])\n",
    "        #lv_t_s = np.concatenate([lv_t_s, np.array(sessions_traces_dict[event_uuid][\"am\"][am_event_index][\"lv_t_s\"])])\n",
    "        mean_fluo = np.concatenate([mean_fluo, np.array(sessions_traces_dict[event_uuid][\"am\"][am_event_index][\"mean_fluo\"])])\n",
    "        #lfp_t = np.concatenate([lfp_t, np.array(sessions_traces_dict[event_uuid][\"am\"][am_event_index][\"lfp_t\"])])\n",
    "        lfp_y = np.concatenate([lfp_y, np.array(sessions_traces_dict[event_uuid][\"am\"][am_event_index][\"lfp_y\"])])\n",
    "        #lfp_mov_t = np.concatenate([lfp_mov_t, np.array(sessions_traces_dict[event_uuid][\"am\"][am_event_index][\"lfp_mov_t\"])])\n",
    "        lfp_mov_y = np.concatenate([lfp_mov_y, np.array(sessions_traces_dict[event_uuid][\"am\"][am_event_index][\"lfp_mov_y\"])])\n",
    "        lv_dt_curr = np.array(create_dt(sessions_traces_dict[event_uuid][\"am\"][am_event_index][\"lv_t_s\"]))\n",
    "        \n",
    "        # match cumulative data to last element\n",
    "        lv_totdist_curr = np.array(sessions_traces_dict[event_uuid][\"am\"][am_event_index][\"lv_totdist\"])\n",
    "        lv_rounds_curr = np.array(sessions_traces_dict[event_uuid][\"am\"][am_event_index][\"lv_rounds\"])\n",
    "        lv_totdist_abs_curr = np.array(create_totdist_abs(lv_speed_curr, lv_dt_curr))\n",
    "        lv_totdist_last = lv_totdist[-1]\n",
    "        lv_rounds_last = lv_rounds[-1]\n",
    "        lv_totdist_abs_last = lv_totdist_abs[-1]\n",
    "        lv_totdist = np.concatenate([lv_totdist, lv_totdist_curr - lv_totdist_curr[0] + lv_totdist_last])\n",
    "        lv_rounds = np.concatenate([lv_rounds, lv_rounds_curr - lv_rounds_curr[0] + lv_rounds_last])\n",
    "        lv_totdist_abs = np.concatenate([lv_totdist_abs, lv_totdist_abs_curr - lv_totdist_abs_curr[0] + lv_totdist_abs_last])\n",
    "        \n",
    "        \n",
    "        lv_t_s_curr = np.array(sessions_traces_dict[event_uuid][\"am\"][am_event_index][\"lv_t_s\"])\n",
    "        lfp_t_curr = np.array(sessions_traces_dict[event_uuid][\"am\"][am_event_index][\"lfp_t\"])\n",
    "        lfp_mov_t_curr = np.array(sessions_traces_dict[event_uuid][\"am\"][am_event_index][\"lfp_mov_t\"])\n",
    "        # use same method of matching to end of previous segment as in bl\n",
    "        dt_lv_t_s_avg = (lv_t_s_curr[1:] - lv_t_s_curr[:-1]).mean()\n",
    "        dt_lfp_t_avg = (lfp_t_curr[1:] - lfp_t_curr[:-1]).mean()\n",
    "        dt_lfp_mov_t_avg = (lfp_mov_t_curr[1:] - lfp_mov_t_curr[:-1]).mean()\n",
    "        \n",
    "        # add break points for segment concatenated below\n",
    "        event_metadata_dict[\"break_points\"].append(len(lv_t_s))\n",
    "        event_metadata_dict[\"break_points_lfp\"].append(len(lfp_t))\n",
    "        current_recording_uuid = df_events[(df_events[\"event_uuid\"] == event_uuid) & (df_events[\"event_index\"] == am_event_index)].recording_uuid.values[0]\n",
    "        if (current_recording_uuid != previous_recording_uuid) and (previous_recording_uuid is not None):\n",
    "            event_metadata_dict[\"recording_break_points\"].append(len(lv_t_s))\n",
    "            event_metadata_dict[\"recording_break_points_lfp\"].append(len(lfp_t))\n",
    "        previous_recording_uuid = current_recording_uuid\n",
    "        \n",
    "        # once we have dt, need to modify arrays by (-current[0] + last[-1] + dt)\n",
    "        lv_t_s = np.concatenate([lv_t_s, lv_t_s_curr - lv_t_s_curr[0] + lv_t_s[-1] + dt_lv_t_s_avg])\n",
    "        lfp_t = np.concatenate([lfp_t, lfp_t_curr - lfp_t_curr[0] + lfp_t[-1] + dt_lfp_t_avg])\n",
    "        lfp_mov_t = np.concatenate([lfp_mov_t, lfp_mov_t_curr - lfp_mov_t_curr[0] + lfp_mov_t[-1] + dt_lfp_mov_t_avg])\n",
    "        \n",
    "        t_last_segment_nik_end = sessions_nik_tstamps_dict[event_uuid][\"am\"][am_event_index][1] \n",
    "        \n",
    "        i_segment += 1\n",
    "        \n",
    "    print()\n",
    "    joint_session_traces_dict[event_uuid] = {\"lv_dist\":lv_dist, \"lv_speed\":lv_speed,\"lv_running\":lv_running,\"lv_totdist\":lv_totdist, \"lv_totdist_abs\": lv_totdist_abs, \"lv_dt\": lv_dt, \"lv_rounds\":lv_rounds,\"lv_t_s\":lv_t_s,\"lfp_t\":lfp_t,\"lfp_y\":lfp_y,\"lfp_mov_t\":lfp_mov_t,\"lfp_mov_y\":lfp_mov_y,\"mean_fluo\":mean_fluo}\n",
    "    joint_session_metadata_dict[event_uuid] = event_metadata_dict.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab456b",
   "metadata": {},
   "source": [
    "### Save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265f0f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_assembled_fpath = os.path.join(output_dir, f\"assembled_traces_{get_datetime_for_fname()}.h5\")\n",
    "print(f\"Saving stitched-together traces to {output_assembled_fpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d794dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(output_assembled_fpath, \"w\") as hf:\n",
    "    for event_uuid in joint_session_traces_dict.keys():\n",
    "        event_uuid_grp = hf.create_group(event_uuid)\n",
    "        df_attributes = df_events[df_events[\"event_uuid\"] == event_uuid]\n",
    "        \n",
    "        event_uuid_grp.attrs[\"session_uuids\"] = df_attributes[\"recording_uuid\"].unique()\n",
    "        event_uuid_grp.attrs[\"has_lfp\"] = [has_lfp_dict[uuid] for uuid in df_attributes[\"recording_uuid\"].unique()]\n",
    "        event_uuid_grp.attrs[\"window_type\"] = df_attributes[\"window_type\"].unique()[0]  # same event should happen for same window!\n",
    "        event_uuid_grp.attrs[\"n_frames\"] = len(joint_session_traces_dict[event_uuid][\"mean_fluo\"])\n",
    "        event_uuid_grp.attrs[\"mouse_id\"] = df_attributes[\"mouse_id\"].unique()[0]\n",
    "        event_uuid_grp.attrs[\"n_lfp_steps\"] = len(joint_session_traces_dict[event_uuid][\"lfp_t\"])\n",
    "        event_uuid_grp.attrs[\"n_lfp_mov_steps\"] = len(joint_session_traces_dict[event_uuid][\"lfp_mov_t\"])\n",
    "        event_uuid_grp.attrs[\"n_bl_frames\"] = LEN_BL_AM_FRAMES\n",
    "        event_uuid_grp.attrs[\"n_am_frames\"] = LEN_BL_AM_FRAMES\n",
    "        \n",
    "        for attr_name in joint_session_metadata_dict[event_uuid].keys():\n",
    "            event_uuid_grp.attrs[attr_name] = joint_session_metadata_dict[event_uuid][attr_name]\n",
    "        \n",
    "        for array_name in joint_session_traces_dict[event_uuid].keys():\n",
    "            event_uuid_grp.create_dataset(array_name, data=joint_session_traces_dict[event_uuid][array_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: TwoPhotonSession should have totdist_abs now. Can simply take it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
